{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-network\n",
    "#!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ChebNet\n",
    "import CORA\n",
    "import utils\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/alice/Documents/GIT/CayleyNets/utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload when files are changed\n",
    "import importlib\n",
    "importlib.reload(ChebNet)\n",
    "importlib.reload(CORA)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Checking torch and device\n",
    "\n",
    "# Torch version\n",
    "print(torch.__version__)\n",
    "\n",
    "# Is MPS even available? macOS 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Was the current version of PyTorch built with MPS activated?\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing files...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "cora = CORA.CORA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask, val_mask = utils.split_train_test_val(cora.n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_feats = cora.n_features\n",
    "n_classes = cora.n_classes\n",
    "n_hidden = 16\n",
    "n_layers = 1 # number of hidden+output layers\n",
    "k = 5 # Chebyshev polynomial order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph as an object\n",
    "graph = dgl.from_scipy(cora.adjacency)\n",
    "\n",
    "# Features and labels as tensors\n",
    "features = torch.Tensor(cora.features).to(device)\n",
    "labels = torch.Tensor(cora.labels).long().to(device)\n",
    "\n",
    "# Masks as tensors\n",
    "train_mask = torch.Tensor(train_mask).bool().to(device)\n",
    "test_mask = torch.Tensor(test_mask).bool().to(device)\n",
    "val_mask = torch.Tensor(val_mask).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChebNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): ChebConv(\n",
       "      (linear): Linear(in_features=7165, out_features=16, bias=True)\n",
       "    )\n",
       "    (1): ChebConv(\n",
       "      (linear): Linear(in_features=80, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model = ChebNet.ChebNet(graph, in_feats, n_classes, n_hidden, n_layers, k, bias=True).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 115223\n",
      "Number of parameters: 114656\n",
      "Number of parameters: 567\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.layers[0].parameters())}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.layers[1].parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, features, labels, test_mask):\n",
    "    '''Evaluate the model in terms of accuracy.'''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features)\n",
    "        labels_pred = torch.max(output, dim=1)[1]\n",
    "        score = np.mean(np.array(labels[test_mask]) == np.array(labels_pred[test_mask]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 1.9454\n",
      "Epoch: 001 | Loss: 1.8889\n",
      "Epoch: 002 | Loss: 1.8144\n",
      "Epoch: 003 | Loss: 1.7448\n",
      "Epoch: 004 | Loss: 1.6902\n",
      "Epoch: 005 | Loss: 1.6250\n",
      "Epoch: 006 | Loss: 1.5478\n",
      "Epoch: 007 | Loss: 1.4789\n",
      "Epoch: 008 | Loss: 1.4212\n",
      "Epoch: 009 | Loss: 1.3689\n",
      "Epoch: 010 | Loss: 1.3227\n",
      "Epoch: 011 | Loss: 1.2853\n",
      "Epoch: 012 | Loss: 1.2566\n",
      "Epoch: 013 | Loss: 1.2294\n",
      "Epoch: 014 | Loss: 1.2030\n",
      "Epoch: 015 | Loss: 1.1796\n",
      "Epoch: 016 | Loss: 1.1607\n",
      "Epoch: 017 | Loss: 1.1443\n",
      "Epoch: 018 | Loss: 1.1284\n",
      "Epoch: 019 | Loss: 1.1136\n",
      "Epoch: 020 | Loss: 1.1004\n",
      "Epoch: 021 | Loss: 1.0881\n",
      "Epoch: 022 | Loss: 1.0762\n",
      "Epoch: 023 | Loss: 1.0647\n",
      "Epoch: 024 | Loss: 1.0538\n",
      "Epoch: 025 | Loss: 1.0436\n",
      "Epoch: 026 | Loss: 1.0343\n",
      "Epoch: 027 | Loss: 1.0213\n",
      "Epoch: 028 | Loss: 1.0019\n",
      "Epoch: 029 | Loss: 0.9824\n",
      "Epoch: 030 | Loss: 0.9673\n",
      "Epoch: 031 | Loss: 0.9567\n",
      "Epoch: 032 | Loss: 0.9466\n",
      "Epoch: 033 | Loss: 0.9336\n",
      "Epoch: 034 | Loss: 0.9175\n",
      "Epoch: 035 | Loss: 0.8999\n",
      "Epoch: 036 | Loss: 0.8824\n",
      "Epoch: 037 | Loss: 0.8656\n",
      "Epoch: 038 | Loss: 0.8488\n",
      "Epoch: 039 | Loss: 0.8313\n",
      "Epoch: 040 | Loss: 0.8129\n",
      "Epoch: 041 | Loss: 0.7944\n",
      "Epoch: 042 | Loss: 0.7766\n",
      "Epoch: 043 | Loss: 0.7600\n",
      "Epoch: 044 | Loss: 0.7456\n",
      "Epoch: 045 | Loss: 0.7330\n",
      "Epoch: 046 | Loss: 0.7222\n",
      "Epoch: 047 | Loss: 0.7139\n",
      "Epoch: 048 | Loss: 0.7071\n",
      "Epoch: 049 | Loss: 0.7011\n",
      "Epoch: 050 | Loss: 0.6955\n",
      "Epoch: 051 | Loss: 0.6905\n",
      "Epoch: 052 | Loss: 0.6863\n",
      "Epoch: 053 | Loss: 0.6826\n",
      "Epoch: 054 | Loss: 0.6795\n",
      "Epoch: 055 | Loss: 0.6769\n",
      "Epoch: 056 | Loss: 0.6745\n",
      "Epoch: 057 | Loss: 0.6723\n",
      "Epoch: 058 | Loss: 0.6703\n",
      "Epoch: 059 | Loss: 0.6683\n",
      "Epoch: 060 | Loss: 0.6663\n",
      "Epoch: 061 | Loss: 0.6644\n",
      "Epoch: 062 | Loss: 0.6625\n",
      "Epoch: 063 | Loss: 0.6607\n",
      "Epoch: 064 | Loss: 0.6590\n",
      "Epoch: 065 | Loss: 0.6574\n",
      "Epoch: 066 | Loss: 0.6459\n",
      "Epoch: 067 | Loss: 0.6368\n",
      "Epoch: 068 | Loss: 0.6343\n",
      "Epoch: 069 | Loss: 0.6288\n",
      "Epoch: 070 | Loss: 0.6201\n",
      "Epoch: 071 | Loss: 0.6126\n",
      "Epoch: 072 | Loss: 0.6064\n",
      "Epoch: 073 | Loss: 0.5978\n",
      "Epoch: 074 | Loss: 0.5865\n",
      "Epoch: 075 | Loss: 0.5751\n",
      "Epoch: 076 | Loss: 0.5635\n",
      "Epoch: 077 | Loss: 0.5504\n",
      "Epoch: 078 | Loss: 0.5360\n",
      "Epoch: 079 | Loss: 0.5235\n",
      "Epoch: 080 | Loss: 0.5137\n",
      "Epoch: 081 | Loss: 0.5050\n",
      "Epoch: 082 | Loss: 0.4970\n",
      "Epoch: 083 | Loss: 0.4906\n",
      "Epoch: 084 | Loss: 0.4858\n",
      "Epoch: 085 | Loss: 0.4824\n",
      "Epoch: 086 | Loss: 0.4784\n",
      "Epoch: 087 | Loss: 0.4748\n",
      "Epoch: 088 | Loss: 0.4721\n",
      "Epoch: 089 | Loss: 0.4696\n",
      "Epoch: 090 | Loss: 0.4671\n",
      "Epoch: 091 | Loss: 0.4645\n",
      "Epoch: 092 | Loss: 0.4623\n",
      "Epoch: 093 | Loss: 0.4602\n",
      "Epoch: 094 | Loss: 0.4582\n",
      "Epoch: 095 | Loss: 0.4562\n",
      "Epoch: 096 | Loss: 0.4544\n",
      "Epoch: 097 | Loss: 0.4528\n",
      "Epoch: 098 | Loss: 0.4513\n",
      "Epoch: 099 | Loss: 0.4498\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# Save loss values for plotting\n",
    "loss_values = []\n",
    "scores = []\n",
    "for e in range(epochs):\n",
    "    # Compute output\n",
    "    output = model(features)\n",
    "\n",
    "    # Compute loss\n",
    "    logp = F.log_softmax(output, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    # Perform backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    score = eval_model(model, graph, features, labels, val_mask)\n",
    "    scores.append(score)\n",
    "\n",
    "    # Print loss\n",
    "    print(f'Epoch: {e:03d} | Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7601476014760148"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(model, features, labels, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
