{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-network\n",
    "#!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ChebNet\n",
    "import CORA\n",
    "import utils\n",
    "\n",
    "import dgl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/alice/Documents/GIT/CayleyNets/utils.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload when files are changed\n",
    "import importlib\n",
    "importlib.reload(ChebNet)\n",
    "importlib.reload(CORA)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Checking torch and device\n",
    "\n",
    "# Torch version\n",
    "print(torch.__version__)\n",
    "\n",
    "# Is MPS even available? macOS 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Was the current version of PyTorch built with MPS activated?\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing files...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "cora = CORA.CORA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask, val_mask = utils.split_train_test_val(cora.n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_feats = cora.n_features\n",
    "n_classes = cora.n_classes\n",
    "n_hidden = 16\n",
    "n_layers = 1 # number of hidden+output layers\n",
    "k = 5 # Chebyshev polynomial order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph as an object\n",
    "graph = dgl.from_scipy(cora.adjacency)\n",
    "\n",
    "# Features and labels as tensors\n",
    "features = torch.Tensor(cora.features).to(device)\n",
    "labels = torch.Tensor(cora.labels).long().to(device)\n",
    "\n",
    "# Masks as tensors\n",
    "train_mask = torch.Tensor(train_mask).bool().to(device)\n",
    "test_mask = torch.Tensor(test_mask).bool().to(device)\n",
    "val_mask = torch.Tensor(val_mask).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChebNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): ChebConv(\n",
       "      (linear): Linear(in_features=7165, out_features=16, bias=True)\n",
       "    )\n",
       "    (1): ChebConv(\n",
       "      (linear): Linear(in_features=80, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model = ChebNet.ChebNet(graph, in_feats, n_classes, n_hidden, n_layers, k, bias=True).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 115223\n",
      "Number of parameters: 114656\n",
      "Number of parameters: 567\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.layers[0].parameters())}')\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.layers[1].parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Loss: 1.9309\n",
      "Epoch: 001 | Loss: 1.8730\n",
      "Epoch: 002 | Loss: 1.8086\n",
      "Epoch: 003 | Loss: 1.7481\n",
      "Epoch: 004 | Loss: 1.6782\n",
      "Epoch: 005 | Loss: 1.6020\n",
      "Epoch: 006 | Loss: 1.5267\n",
      "Epoch: 007 | Loss: 1.4562\n",
      "Epoch: 008 | Loss: 1.3918\n",
      "Epoch: 009 | Loss: 1.3350\n",
      "Epoch: 010 | Loss: 1.2884\n",
      "Epoch: 011 | Loss: 1.2517\n",
      "Epoch: 012 | Loss: 1.2201\n",
      "Epoch: 013 | Loss: 1.1930\n",
      "Epoch: 014 | Loss: 1.1701\n",
      "Epoch: 015 | Loss: 1.1497\n",
      "Epoch: 016 | Loss: 1.1309\n",
      "Epoch: 017 | Loss: 1.1133\n",
      "Epoch: 018 | Loss: 1.0968\n",
      "Epoch: 019 | Loss: 1.0814\n",
      "Epoch: 020 | Loss: 1.0671\n",
      "Epoch: 021 | Loss: 1.0534\n",
      "Epoch: 022 | Loss: 1.0407\n",
      "Epoch: 023 | Loss: 1.0293\n",
      "Epoch: 024 | Loss: 1.0186\n",
      "Epoch: 025 | Loss: 1.0087\n",
      "Epoch: 026 | Loss: 0.9997\n",
      "Epoch: 027 | Loss: 0.9913\n",
      "Epoch: 028 | Loss: 0.9835\n",
      "Epoch: 029 | Loss: 0.9763\n",
      "Epoch: 030 | Loss: 0.9698\n",
      "Epoch: 031 | Loss: 0.9639\n",
      "Epoch: 032 | Loss: 0.9586\n",
      "Epoch: 033 | Loss: 0.9539\n",
      "Epoch: 034 | Loss: 0.9500\n",
      "Epoch: 035 | Loss: 0.9465\n",
      "Epoch: 036 | Loss: 0.9435\n",
      "Epoch: 037 | Loss: 0.9408\n",
      "Epoch: 038 | Loss: 0.9385\n",
      "Epoch: 039 | Loss: 0.9365\n",
      "Epoch: 040 | Loss: 0.9347\n",
      "Epoch: 041 | Loss: 0.9331\n",
      "Epoch: 042 | Loss: 0.9317\n",
      "Epoch: 043 | Loss: 0.9305\n",
      "Epoch: 044 | Loss: 0.9293\n",
      "Epoch: 045 | Loss: 0.9283\n",
      "Epoch: 046 | Loss: 0.9274\n",
      "Epoch: 047 | Loss: 0.9266\n",
      "Epoch: 048 | Loss: 0.9260\n",
      "Epoch: 049 | Loss: 0.9255\n",
      "Epoch: 050 | Loss: 0.9251\n",
      "Epoch: 051 | Loss: 0.9051\n",
      "Epoch: 052 | Loss: 0.8882\n",
      "Epoch: 053 | Loss: 0.8830\n",
      "Epoch: 054 | Loss: 0.8849\n",
      "Epoch: 055 | Loss: 0.8760\n",
      "Epoch: 056 | Loss: 0.8617\n",
      "Epoch: 057 | Loss: 0.8513\n",
      "Epoch: 058 | Loss: 0.8435\n",
      "Epoch: 059 | Loss: 0.8321\n",
      "Epoch: 060 | Loss: 0.8151\n",
      "Epoch: 061 | Loss: 0.7952\n",
      "Epoch: 062 | Loss: 0.7758\n",
      "Epoch: 063 | Loss: 0.7571\n",
      "Epoch: 064 | Loss: 0.7381\n",
      "Epoch: 065 | Loss: 0.7190\n",
      "Epoch: 066 | Loss: 0.7018\n",
      "Epoch: 067 | Loss: 0.6889\n",
      "Epoch: 068 | Loss: 0.6804\n",
      "Epoch: 069 | Loss: 0.6733\n",
      "Epoch: 070 | Loss: 0.6655\n",
      "Epoch: 071 | Loss: 0.6578\n",
      "Epoch: 072 | Loss: 0.6522\n",
      "Epoch: 073 | Loss: 0.6479\n",
      "Epoch: 074 | Loss: 0.6448\n",
      "Epoch: 075 | Loss: 0.6414\n",
      "Epoch: 076 | Loss: 0.6380\n",
      "Epoch: 077 | Loss: 0.6354\n",
      "Epoch: 078 | Loss: 0.6334\n",
      "Epoch: 079 | Loss: 0.6316\n",
      "Epoch: 080 | Loss: 0.6297\n",
      "Epoch: 081 | Loss: 0.6279\n",
      "Epoch: 082 | Loss: 0.6262\n",
      "Epoch: 083 | Loss: 0.6250\n",
      "Epoch: 084 | Loss: 0.6239\n",
      "Epoch: 085 | Loss: 0.6228\n",
      "Epoch: 086 | Loss: 0.6218\n",
      "Epoch: 087 | Loss: 0.6208\n",
      "Epoch: 088 | Loss: 0.6201\n",
      "Epoch: 089 | Loss: 0.6195\n",
      "Epoch: 090 | Loss: 0.6188\n",
      "Epoch: 091 | Loss: 0.6183\n",
      "Epoch: 092 | Loss: 0.6178\n",
      "Epoch: 093 | Loss: 0.6175\n",
      "Epoch: 094 | Loss: 0.6171\n",
      "Epoch: 095 | Loss: 0.6168\n",
      "Epoch: 096 | Loss: 0.6165\n",
      "Epoch: 097 | Loss: 0.6164\n",
      "Epoch: 098 | Loss: 0.6162\n",
      "Epoch: 099 | Loss: 0.6160\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "# save loss values for plotting\n",
    "loss_values = []\n",
    "for e in range(epochs):\n",
    "    # Compute output\n",
    "    output = model(features)\n",
    "\n",
    "    # Compute loss\n",
    "    logp = F.log_softmax(output, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    # Perform backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    print(f'Epoch: {e:03d} | Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
