{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import dgl\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from coarsening import coarsen\n",
    "from coordinate import get_coordinates, z2polar\n",
    "from dgl.data import load_data, register_data_args\n",
    "from dgl.nn.pytorch.conv import ChebConv, GMMConv\n",
    "from dgl.nn.pytorch.glob import MaxPooling\n",
    "from grid_graph import grid_graph\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import CayleyConv\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "reload(CayleyConv)\n",
    "\n",
    "gpu = 1\n",
    "net = 'cayleynet'\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb edges:  6396\n",
      "Heavy Edge Matching coarsening with Xavier version\n",
      "Layer 0: M_0 = |V| = 960 nodes (176 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 480 nodes (79 added), |E| = 1603 edges\n",
      "Layer 2: M_2 = |V| = 240 nodes (31 added), |E| = 776 edges\n",
      "Layer 3: M_3 = |V| = 120 nodes (7 added), |E| = 392 edges\n",
      "Layer 4: M_4 = |V| = 60 nodes (0 added), |E| = 198 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_side = 28\n",
    "number_edges = 8\n",
    "metric = \"euclidean\"\n",
    "\n",
    "A = grid_graph(28, 8, metric)\n",
    "\n",
    "coarsening_levels = 4\n",
    "L, perm = coarsen(A, coarsening_levels)\n",
    "g_arr = [dgl.from_scipy(csr) for csr in L]\n",
    "\n",
    "coordinate_arr = get_coordinates(g_arr, grid_side, coarsening_levels, perm)\n",
    "str_to_torch_dtype = {\n",
    "    \"float16\": torch.half,\n",
    "    \"float32\": torch.float32,\n",
    "    \"float64\": torch.float64,\n",
    "}\n",
    "coordinate_arr = [\n",
    "    coord.to(dtype=str_to_torch_dtype[str(A.dtype)]) for coord in coordinate_arr\n",
    "]\n",
    "for g, coordinate_arr in zip(g_arr, coordinate_arr):\n",
    "    g.ndata[\"xy\"] = coordinate_arr\n",
    "    g.apply_edges(z2polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batcher(batch):\n",
    "    ''' \n",
    "    Converts a batch of (graph, feature, label) tuples to a\n",
    "    batch of (graph, feature, label) tuples for each coarsening level.\n",
    "    '''\n",
    "    g_batch = [[] for _ in range(coarsening_levels + 1)]\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for x, y in batch:\n",
    "        x = torch.cat([x.view(-1), x.new_zeros(len(perm) - 28**2)], 0)\n",
    "        x = x[perm]\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "        for i in range(coarsening_levels + 1):\n",
    "            g_batch[i].append(g_arr[i])\n",
    "\n",
    "    x_batch = torch.cat(x_batch).unsqueeze(-1)\n",
    "    y_batch = torch.LongTensor(y_batch)\n",
    "    g_batch = [dgl.batch(g) for g in g_batch]\n",
    "    return g_batch, x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = datasets.MNIST(\n",
    "    root=\".\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "testset = datasets.MNIST(\n",
    "    root=\".\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=batcher,\n",
    "    num_workers=6,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=batcher,\n",
    "    num_workers=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CayletNet(nn.Module):\n",
    "    def __init__(self, k, in_feats, hiddens, out_feats):\n",
    "        super(CayletNet, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.readout = MaxPooling()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(CayleyConv.CayleyConv(in_feats, hiddens[0], k))\n",
    "\n",
    "        for i in range(1, len(hiddens)):\n",
    "            self.layers.append(CayleyConv.CayleyConv(hiddens[i - 1], hiddens[i], k))\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(hiddens[-1], out_feats), nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, g_arr, feat):\n",
    "        for g, layer in zip(g_arr, self.layers):\n",
    "            # LongTensor of shape (2, number_edges)\n",
    "            edges_index = torch.zeros((2, g.number_of_edges()), dtype=torch.long, device=g.device)\n",
    "            edges_index[0] = g.edges()[0]\n",
    "            edges_index[1] = g.edges()[1]\n",
    "            feat = (\n",
    "                self.pool(\n",
    "                    layer(feat, edges_index) #, [2] * g.batch_size)\n",
    "                    .transpose(-1, -2)\n",
    "                    .unsqueeze(0)\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .transpose(-1, -2)\n",
    "            )\n",
    "        return self.cls(self.readout(g_arr[-1], feat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MoNet(nn.Module):\n",
    "    def __init__(self, n_kernels, in_feats, hiddens, out_feats):\n",
    "        super(MoNet, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.readout = MaxPooling()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(GMMConv(in_feats, hiddens[0], 2, n_kernels))\n",
    "\n",
    "        # Hidden layer\n",
    "        for i in range(1, len(hiddens)):\n",
    "            self.layers.append(\n",
    "                GMMConv(hiddens[i - 1], hiddens[i], 2, n_kernels)\n",
    "            )\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(hiddens[-1], out_feats), nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, g_arr, feat):\n",
    "        for g, layer in zip(g_arr, self.layers):\n",
    "            u = g.edata[\"u\"]\n",
    "            feat = (\n",
    "                self.pool(layer(g, feat, u).transpose(-1, -2).unsqueeze(0))\n",
    "                .squeeze(0)\n",
    "                .transpose(-1, -2)\n",
    "            )\n",
    "        return self.cls(self.readout(g_arr[-1], feat))\n",
    "\n",
    "\n",
    "class ChebNet(nn.Module):\n",
    "    def __init__(self, k, in_feats, hiddens, out_feats):\n",
    "        super(ChebNet, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.readout = MaxPooling()\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(ChebConv(in_feats, hiddens[0], k))\n",
    "\n",
    "        for i in range(1, len(hiddens)):\n",
    "            self.layers.append(ChebConv(hiddens[i - 1], hiddens[i], k))\n",
    "\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(hiddens[-1], out_feats), nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, g_arr, feat):\n",
    "        for g, layer in zip(g_arr, self.layers):\n",
    "            feat = (\n",
    "                self.pool(\n",
    "                    layer(g, feat, [2] * g.batch_size)\n",
    "                    .transpose(-1, -2)\n",
    "                    .unsqueeze(0)\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .transpose(-1, -2)\n",
    "            )\n",
    "        return self.cls(self.readout(g_arr[-1], feat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 starts\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 68.65 GiB (GPU 1; 31.75 GiB total capacity; 95.43 MiB already allocated; 30.16 GiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/infres/jalvarez-22/GDA/project/mnist.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m g \u001b[39m=\u001b[39m [g_i\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m g_i \u001b[39min\u001b[39;00m g]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m out \u001b[39m=\u001b[39m model(g, x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m hit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (out\u001b[39m.\u001b[39mmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m tot \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/infres/jalvarez-22/GDA/project/mnist.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     edges_index[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39medges()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     edges_index[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39medges()[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     feat \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m             layer(feat, edges_index) \u001b[39m#, [2] * g.batch_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m             \u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m             \u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22677075322e656e73742e6672222c2275736572223a226a616c766172657a2d3232227d/home/infres/jalvarez-22/GDA/project/mnist.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadout(g_arr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], feat))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GDA/project/CayleyConv.py:170\u001b[0m, in \u001b[0;36mCayleyConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    168\u001b[0m     B \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msparse_coo_tensor(B[\u001b[39m0\u001b[39m], B[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mcoalesce()\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     L \u001b[39m=\u001b[39m to_dense_adj(edge_index \u001b[39m=\u001b[39;49m L[\u001b[39m0\u001b[39;49m], edge_attr \u001b[39m=\u001b[39;49m L[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    171\u001b[0m     \u001b[39m# L is of size (1, N, N)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m*\u001b[39m L\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/utils/to_dense_adj.py:98\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m flattened_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m max_num_nodes \u001b[39m*\u001b[39m max_num_nodes\n\u001b[1;32m     97\u001b[0m idx \u001b[39m=\u001b[39m idx0 \u001b[39m*\u001b[39m max_num_nodes \u001b[39m*\u001b[39m max_num_nodes \u001b[39m+\u001b[39m idx1 \u001b[39m*\u001b[39m max_num_nodes \u001b[39m+\u001b[39m idx2\n\u001b[0;32m---> 98\u001b[0m adj \u001b[39m=\u001b[39m scatter(edge_attr, idx, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, dim_size\u001b[39m=\u001b[39;49mflattened_size, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     99\u001b[0m adj \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39mview(size)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m adj\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:70\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     69\u001b[0m     index \u001b[39m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mnew_zeros(size)\u001b[39m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     count \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 68.65 GiB (GPU 1; 31.75 GiB total capacity; 95.43 MiB already allocated; 30.16 GiB free; 112.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "if gpu == -1:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(gpu)\n",
    "\n",
    "if net == \"chebnet\":\n",
    "    model = ChebNet(2, 1, [32, 64, 128, 256], 10)\n",
    "elif net == \"monet\":\n",
    "    model = MoNet(10, 1, [32, 64, 128, 256], 10)\n",
    "elif net == \"cayleynet\":\n",
    "    model = CayletNet(2, 1, [32, 32], 6)\n",
    "else :\n",
    "    print(\"net not implemented, please choose from [chebnet, monet, cayleynet]\")\n",
    "    raise NotImplementedError \n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "log_interval = 50\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"epoch {} starts\".format(epoch))\n",
    "    model.train()\n",
    "    hit, tot = 0, 0\n",
    "    loss_accum = 0\n",
    "    for i, (g, x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        g = [g_i.to(device) for g_i in g]\n",
    "        out = model(g, x)\n",
    "        hit += (out.max(-1)[1] == y).sum().item()\n",
    "        tot += len(y)\n",
    "        loss = F.nll_loss(out, y)\n",
    "        loss_accum += loss.item()\n",
    "\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(\n",
    "                \"loss: {}, acc: {}\".format(loss_accum / log_interval, hit / tot)\n",
    "            )\n",
    "            hit, tot = 0, 0\n",
    "            loss_accum = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    hit, tot = 0, 0\n",
    "    for g, x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        g = [g_i.to(device) for g_i in g]\n",
    "        out = model(g, x)\n",
    "        hit += (out.max(-1)[1] == y).sum().item()\n",
    "        tot += len(y)\n",
    "\n",
    "    print(\"test acc: \", hit / tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
