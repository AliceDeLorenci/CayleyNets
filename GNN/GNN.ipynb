{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN for graph classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is based entirely on the TP of the ALTEGRAD 2023 class for the MVA - ENS Paris-Saclay**\n",
    "\n",
    "Lecture: Prof. Michalis Vazirgiannis\n",
    "\n",
    "Lab: Dr. Giannis Nikolentzos & Dr. Johannes Lutzeyer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from random import randint\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.sparse import diags, eye\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import time\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    # Ãƒ = A + I\n",
    "    A_tilde = A + sp.identity(A.shape[0])\n",
    "\n",
    "    # degree matrix D\n",
    "    D = np.sum(A_tilde, axis=1)\n",
    "    D_inv_sqrt = 1 / np.sqrt(D)\n",
    "    D_inv_sqrt = np.squeeze(np.asarray(D_inv_sqrt))\n",
    "    D_inv_sqrt_matrix = sp.diags(D_inv_sqrt, format='csc')\n",
    "\n",
    "    # Normalized adjacency matrix A_normalized\n",
    "    A_normalized = D_inv_sqrt_matrix @ A_tilde @ D_inv_sqrt_matrix\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "\n",
    "def load_cora():\n",
    "    idx_features_labels = np.genfromtxt(\"./data/cora.content\", dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    features = features.todense()\n",
    "    features /= features.sum(1).reshape(-1, 1)\n",
    "    \n",
    "    class_labels = idx_features_labels[:, -1]\n",
    "    le = LabelEncoder()\n",
    "    class_labels = le.fit_transform(class_labels)\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"./data/cora.cites\", dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(class_labels.size, class_labels.size), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    return features, adj, class_labels\n",
    "\n",
    "\n",
    "def sparse_to_torch_sparse(M):\n",
    "    \"\"\"Converts a sparse SciPy matrix to a sparse PyTorch tensor\"\"\"\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((M.row, M.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\" taken from LAB 5 on graph neural networks of the cours ALTEGRAD from MVA 2023 \n",
    "        Lecture: Prof. Michalis Vazirgiannis\n",
    "        Lab: Dr. Giannis Nikolentzos & Dr. Johannes Lutzeyer \"\"\"\n",
    "    def __init__(self, n_feat, n_hidden, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_class)\n",
    "       # self.fc3 = nn.Linear(n_hidden_2, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj):\n",
    "        \n",
    "        # First layer\n",
    "        z1 = self.fc1(x_in)\n",
    "        h1 = self.relu(torch.mm(adj, z1))\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.fc2(h1)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), h1 #, h2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "n_hidden = 32\n",
    "learning_rate = 5e-3\n",
    "weight_decay = 5e-4\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Read data\n",
    "features, adj, class_labels = load_cora()\n",
    "n = adj.shape[0] # Number of nodes\n",
    "n_class = np.unique(class_labels).size # Number of classes\n",
    "\n",
    "adj = normalize_adjacency(adj) # Normalize adjacency matrix\n",
    "\n",
    "# Yields indices to split data into training, validation and test sets\n",
    "idx = np.random.permutation(n)\n",
    "idx_train = idx[:int(0.6*n)]\n",
    "idx_val = idx[int(0.6*n):int(0.8*n)]\n",
    "idx_test = idx[int(0.8*n):]\n",
    "\n",
    "# Transform the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features).to(device)\n",
    "y = torch.LongTensor(class_labels).to(device)\n",
    "adj = sparse_to_torch_sparse(adj).to(device)\n",
    "idx_train = torch.LongTensor(idx_train).to(device)\n",
    "idx_val = torch.LongTensor(idx_val).to(device)\n",
    "idx_test = torch.LongTensor(idx_test).to(device)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (fc1): Linear(in_features=1433, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=7, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total number of parameters:  46119\n"
     ]
    }
   ],
   "source": [
    "# Print model's parameters\n",
    "print(model)\n",
    "\n",
    "print('Total number of parameters: ', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model):\n",
    "      t = time.time()\n",
    "      model.train()\n",
    "      optimizer.zero_grad()\n",
    "      output,_ = model(features, adj)\n",
    "      loss_train = F.nll_loss(output[idx_train], y[idx_train])\n",
    "      acc_train = accuracy_score(torch.argmax(output[idx_train], dim=1).detach().cpu().numpy(), y[idx_train].cpu().numpy())\n",
    "      loss_train.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "\n",
    "      model.eval()\n",
    "      output,_ = model(features, adj)\n",
    "\n",
    "      loss_val = F.nll_loss(output[idx_val], y[idx_val])\n",
    "      acc_val = accuracy_score(torch.argmax(output[idx_val], dim=1).detach().cpu().numpy(), y[idx_val].cpu().numpy())\n",
    "      print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "            'acc_train: {:.4f}'.format(acc_train),\n",
    "            'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "            'acc_val: {:.4f}'.format(acc_val),\n",
    "            'time: {:.4f}s'.format(time.time() - t))\n",
    "      return loss_val, acc_val\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      output, embeddings = model(features, adj)\n",
    "      loss_test = F.nll_loss(output[idx_test], y[idx_test])\n",
    "      acc_test = accuracy_score(torch.argmax(output[idx_test], dim=1).detach().cpu().numpy(), y[idx_test].cpu().numpy())\n",
    "\n",
    "      print(\"Test set results:\",\n",
    "            \"loss= {:.4f}\".format(loss_test.item()),\n",
    "            \"accuracy= {:.4f}\".format(acc_test))\n",
    "\n",
    "      \n",
    "      return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for the 1th time\n",
      "Epoch: 001 loss_train: 1.9310 acc_train: 0.0856 loss_val: 1.9255 acc_val: 0.3118 time: 0.0076s\n",
      "Epoch: 002 loss_train: 1.9264 acc_train: 0.2605 loss_val: 1.9208 acc_val: 0.3118 time: 0.0049s\n",
      "Epoch: 003 loss_train: 1.9216 acc_train: 0.2950 loss_val: 1.9161 acc_val: 0.3118 time: 0.0044s\n",
      "Epoch: 004 loss_train: 1.9163 acc_train: 0.2950 loss_val: 1.9111 acc_val: 0.3118 time: 0.0039s\n",
      "Epoch: 005 loss_train: 1.9120 acc_train: 0.2950 loss_val: 1.9057 acc_val: 0.3118 time: 0.0039s\n",
      "Epoch: 006 loss_train: 1.9058 acc_train: 0.2950 loss_val: 1.8999 acc_val: 0.3118 time: 0.0037s\n",
      "Epoch: 007 loss_train: 1.9008 acc_train: 0.2950 loss_val: 1.8938 acc_val: 0.3118 time: 0.0038s\n",
      "Epoch: 008 loss_train: 1.8954 acc_train: 0.2950 loss_val: 1.8875 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 009 loss_train: 1.8876 acc_train: 0.2950 loss_val: 1.8811 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 010 loss_train: 1.8812 acc_train: 0.2950 loss_val: 1.8744 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 011 loss_train: 1.8742 acc_train: 0.2950 loss_val: 1.8676 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 012 loss_train: 1.8681 acc_train: 0.2950 loss_val: 1.8607 acc_val: 0.3118 time: 0.0037s\n",
      "Epoch: 013 loss_train: 1.8619 acc_train: 0.2950 loss_val: 1.8536 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 014 loss_train: 1.8543 acc_train: 0.2950 loss_val: 1.8465 acc_val: 0.3118 time: 0.0038s\n",
      "Epoch: 015 loss_train: 1.8469 acc_train: 0.2950 loss_val: 1.8394 acc_val: 0.3118 time: 0.0037s\n",
      "Epoch: 016 loss_train: 1.8371 acc_train: 0.2950 loss_val: 1.8323 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 017 loss_train: 1.8333 acc_train: 0.2950 loss_val: 1.8253 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 018 loss_train: 1.8257 acc_train: 0.2950 loss_val: 1.8185 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 019 loss_train: 1.8153 acc_train: 0.2950 loss_val: 1.8118 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 020 loss_train: 1.8124 acc_train: 0.2950 loss_val: 1.8052 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 021 loss_train: 1.8038 acc_train: 0.2950 loss_val: 1.7989 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 022 loss_train: 1.7973 acc_train: 0.2950 loss_val: 1.7928 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 023 loss_train: 1.7938 acc_train: 0.2950 loss_val: 1.7868 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 024 loss_train: 1.7887 acc_train: 0.2950 loss_val: 1.7810 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 025 loss_train: 1.7815 acc_train: 0.2950 loss_val: 1.7753 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 026 loss_train: 1.7747 acc_train: 0.2950 loss_val: 1.7697 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 027 loss_train: 1.7730 acc_train: 0.2950 loss_val: 1.7640 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 028 loss_train: 1.7622 acc_train: 0.2950 loss_val: 1.7583 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 029 loss_train: 1.7619 acc_train: 0.2950 loss_val: 1.7525 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 030 loss_train: 1.7558 acc_train: 0.2950 loss_val: 1.7466 acc_val: 0.3118 time: 0.0036s\n",
      "Epoch: 031 loss_train: 1.7527 acc_train: 0.2950 loss_val: 1.7406 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 032 loss_train: 1.7451 acc_train: 0.2950 loss_val: 1.7343 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 033 loss_train: 1.7369 acc_train: 0.2950 loss_val: 1.7279 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 034 loss_train: 1.7289 acc_train: 0.2950 loss_val: 1.7213 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 035 loss_train: 1.7269 acc_train: 0.2962 loss_val: 1.7145 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 036 loss_train: 1.7196 acc_train: 0.2980 loss_val: 1.7076 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 037 loss_train: 1.7145 acc_train: 0.2956 loss_val: 1.7006 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 038 loss_train: 1.7070 acc_train: 0.2974 loss_val: 1.6935 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 039 loss_train: 1.6976 acc_train: 0.2980 loss_val: 1.6863 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 040 loss_train: 1.6919 acc_train: 0.2993 loss_val: 1.6790 acc_val: 0.3137 time: 0.0036s\n",
      "Epoch: 041 loss_train: 1.6795 acc_train: 0.3079 loss_val: 1.6717 acc_val: 0.3137 time: 0.0034s\n",
      "Epoch: 042 loss_train: 1.6759 acc_train: 0.3097 loss_val: 1.6643 acc_val: 0.3137 time: 0.0033s\n",
      "Epoch: 043 loss_train: 1.6768 acc_train: 0.3067 loss_val: 1.6569 acc_val: 0.3137 time: 0.0033s\n",
      "Epoch: 044 loss_train: 1.6624 acc_train: 0.3085 loss_val: 1.6495 acc_val: 0.3155 time: 0.0031s\n",
      "Epoch: 045 loss_train: 1.6558 acc_train: 0.3177 loss_val: 1.6420 acc_val: 0.3173 time: 0.0031s\n",
      "Epoch: 046 loss_train: 1.6467 acc_train: 0.3227 loss_val: 1.6343 acc_val: 0.3229 time: 0.0031s\n",
      "Epoch: 047 loss_train: 1.6405 acc_train: 0.3245 loss_val: 1.6265 acc_val: 0.3284 time: 0.0032s\n",
      "Epoch: 048 loss_train: 1.6363 acc_train: 0.3264 loss_val: 1.6185 acc_val: 0.3321 time: 0.0032s\n",
      "Epoch: 049 loss_train: 1.6260 acc_train: 0.3307 loss_val: 1.6103 acc_val: 0.3376 time: 0.0031s\n",
      "Epoch: 050 loss_train: 1.6158 acc_train: 0.3368 loss_val: 1.6021 acc_val: 0.3413 time: 0.0034s\n",
      "Epoch: 051 loss_train: 1.6038 acc_train: 0.3479 loss_val: 1.5937 acc_val: 0.3450 time: 0.0034s\n",
      "Epoch: 052 loss_train: 1.6043 acc_train: 0.3454 loss_val: 1.5853 acc_val: 0.3579 time: 0.0032s\n",
      "Epoch: 053 loss_train: 1.5902 acc_train: 0.3584 loss_val: 1.5767 acc_val: 0.3672 time: 0.0031s\n",
      "Epoch: 054 loss_train: 1.5821 acc_train: 0.3627 loss_val: 1.5680 acc_val: 0.3672 time: 0.0031s\n",
      "Epoch: 055 loss_train: 1.5726 acc_train: 0.3676 loss_val: 1.5592 acc_val: 0.3690 time: 0.0035s\n",
      "Epoch: 056 loss_train: 1.5706 acc_train: 0.3787 loss_val: 1.5504 acc_val: 0.3745 time: 0.0031s\n",
      "Epoch: 057 loss_train: 1.5562 acc_train: 0.3867 loss_val: 1.5416 acc_val: 0.3875 time: 0.0033s\n",
      "Epoch: 058 loss_train: 1.5413 acc_train: 0.3849 loss_val: 1.5329 acc_val: 0.3911 time: 0.0033s\n",
      "Epoch: 059 loss_train: 1.5400 acc_train: 0.4039 loss_val: 1.5241 acc_val: 0.3967 time: 0.0032s\n",
      "Epoch: 060 loss_train: 1.5292 acc_train: 0.4083 loss_val: 1.5153 acc_val: 0.4022 time: 0.0033s\n",
      "Epoch: 061 loss_train: 1.5219 acc_train: 0.4181 loss_val: 1.5064 acc_val: 0.4096 time: 0.0032s\n",
      "Epoch: 062 loss_train: 1.5236 acc_train: 0.4150 loss_val: 1.4974 acc_val: 0.4188 time: 0.0031s\n",
      "Epoch: 063 loss_train: 1.5075 acc_train: 0.4353 loss_val: 1.4885 acc_val: 0.4262 time: 0.0031s\n",
      "Epoch: 064 loss_train: 1.5002 acc_train: 0.4372 loss_val: 1.4796 acc_val: 0.4317 time: 0.0032s\n",
      "Epoch: 065 loss_train: 1.4823 acc_train: 0.4446 loss_val: 1.4706 acc_val: 0.4428 time: 0.0034s\n",
      "Epoch: 066 loss_train: 1.4785 acc_train: 0.4631 loss_val: 1.4616 acc_val: 0.4557 time: 0.0033s\n",
      "Epoch: 067 loss_train: 1.4697 acc_train: 0.4711 loss_val: 1.4526 acc_val: 0.4649 time: 0.0032s\n",
      "Epoch: 068 loss_train: 1.4612 acc_train: 0.4631 loss_val: 1.4437 acc_val: 0.4742 time: 0.0033s\n",
      "Epoch: 069 loss_train: 1.4579 acc_train: 0.4754 loss_val: 1.4348 acc_val: 0.4871 time: 0.0032s\n",
      "Epoch: 070 loss_train: 1.4460 acc_train: 0.4735 loss_val: 1.4260 acc_val: 0.4889 time: 0.0032s\n",
      "Epoch: 071 loss_train: 1.4364 acc_train: 0.4963 loss_val: 1.4173 acc_val: 0.5000 time: 0.0033s\n",
      "Epoch: 072 loss_train: 1.4260 acc_train: 0.4932 loss_val: 1.4086 acc_val: 0.5166 time: 0.0031s\n",
      "Epoch: 073 loss_train: 1.4253 acc_train: 0.5074 loss_val: 1.4000 acc_val: 0.5258 time: 0.0034s\n",
      "Epoch: 074 loss_train: 1.4158 acc_train: 0.5080 loss_val: 1.3915 acc_val: 0.5314 time: 0.0036s\n",
      "Epoch: 075 loss_train: 1.4117 acc_train: 0.5271 loss_val: 1.3831 acc_val: 0.5314 time: 0.0032s\n",
      "Epoch: 076 loss_train: 1.3979 acc_train: 0.5222 loss_val: 1.3748 acc_val: 0.5369 time: 0.0032s\n",
      "Epoch: 077 loss_train: 1.3888 acc_train: 0.5382 loss_val: 1.3665 acc_val: 0.5424 time: 0.0032s\n",
      "Epoch: 078 loss_train: 1.3852 acc_train: 0.5326 loss_val: 1.3583 acc_val: 0.5461 time: 0.0034s\n",
      "Epoch: 079 loss_train: 1.3748 acc_train: 0.5369 loss_val: 1.3502 acc_val: 0.5517 time: 0.0034s\n",
      "Epoch: 080 loss_train: 1.3759 acc_train: 0.5271 loss_val: 1.3423 acc_val: 0.5535 time: 0.0034s\n",
      "Epoch: 081 loss_train: 1.3598 acc_train: 0.5388 loss_val: 1.3345 acc_val: 0.5572 time: 0.0034s\n",
      "Epoch: 082 loss_train: 1.3542 acc_train: 0.5271 loss_val: 1.3267 acc_val: 0.5572 time: 0.0032s\n",
      "Epoch: 083 loss_train: 1.3455 acc_train: 0.5456 loss_val: 1.3190 acc_val: 0.5572 time: 0.0031s\n",
      "Epoch: 084 loss_train: 1.3358 acc_train: 0.5591 loss_val: 1.3114 acc_val: 0.5646 time: 0.0032s\n",
      "Epoch: 085 loss_train: 1.3276 acc_train: 0.5640 loss_val: 1.3038 acc_val: 0.5720 time: 0.0032s\n",
      "Epoch: 086 loss_train: 1.3220 acc_train: 0.5659 loss_val: 1.2962 acc_val: 0.5849 time: 0.0031s\n",
      "Epoch: 087 loss_train: 1.3153 acc_train: 0.5733 loss_val: 1.2887 acc_val: 0.5867 time: 0.0032s\n",
      "Epoch: 088 loss_train: 1.3125 acc_train: 0.5831 loss_val: 1.2813 acc_val: 0.6015 time: 0.0034s\n",
      "Epoch: 089 loss_train: 1.3091 acc_train: 0.5665 loss_val: 1.2739 acc_val: 0.6052 time: 0.0034s\n",
      "Epoch: 090 loss_train: 1.2979 acc_train: 0.5794 loss_val: 1.2666 acc_val: 0.6255 time: 0.0034s\n",
      "Epoch: 091 loss_train: 1.2915 acc_train: 0.5874 loss_val: 1.2592 acc_val: 0.6273 time: 0.0031s\n",
      "Epoch: 092 loss_train: 1.2877 acc_train: 0.5813 loss_val: 1.2519 acc_val: 0.6347 time: 0.0031s\n",
      "Epoch: 093 loss_train: 1.2783 acc_train: 0.5881 loss_val: 1.2445 acc_val: 0.6384 time: 0.0031s\n",
      "Epoch: 094 loss_train: 1.2761 acc_train: 0.5979 loss_val: 1.2373 acc_val: 0.6402 time: 0.0031s\n",
      "Epoch: 095 loss_train: 1.2748 acc_train: 0.5844 loss_val: 1.2302 acc_val: 0.6421 time: 0.0031s\n",
      "Epoch: 096 loss_train: 1.2634 acc_train: 0.5985 loss_val: 1.2232 acc_val: 0.6439 time: 0.0034s\n",
      "Epoch: 097 loss_train: 1.2459 acc_train: 0.6225 loss_val: 1.2162 acc_val: 0.6439 time: 0.0031s\n",
      "Epoch: 098 loss_train: 1.2391 acc_train: 0.6096 loss_val: 1.2093 acc_val: 0.6458 time: 0.0034s\n",
      "Epoch: 099 loss_train: 1.2347 acc_train: 0.6145 loss_val: 1.2023 acc_val: 0.6494 time: 0.0035s\n",
      "Epoch: 100 loss_train: 1.2296 acc_train: 0.6102 loss_val: 1.1954 acc_val: 0.6568 time: 0.0033s\n",
      "Epoch: 101 loss_train: 1.2146 acc_train: 0.6244 loss_val: 1.1885 acc_val: 0.6624 time: 0.0032s\n",
      "Epoch: 102 loss_train: 1.2252 acc_train: 0.6225 loss_val: 1.1816 acc_val: 0.6661 time: 0.0031s\n",
      "Epoch: 103 loss_train: 1.2121 acc_train: 0.6305 loss_val: 1.1747 acc_val: 0.6808 time: 0.0034s\n",
      "Epoch: 104 loss_train: 1.2094 acc_train: 0.6447 loss_val: 1.1679 acc_val: 0.6845 time: 0.0032s\n",
      "Epoch: 105 loss_train: 1.2052 acc_train: 0.6466 loss_val: 1.1613 acc_val: 0.6882 time: 0.0032s\n",
      "Epoch: 106 loss_train: 1.1866 acc_train: 0.6564 loss_val: 1.1547 acc_val: 0.6919 time: 0.0035s\n",
      "Epoch: 107 loss_train: 1.1889 acc_train: 0.6521 loss_val: 1.1482 acc_val: 0.6937 time: 0.0036s\n",
      "Epoch: 108 loss_train: 1.1857 acc_train: 0.6663 loss_val: 1.1420 acc_val: 0.6974 time: 0.0031s\n",
      "Epoch: 109 loss_train: 1.1842 acc_train: 0.6638 loss_val: 1.1357 acc_val: 0.6974 time: 0.0031s\n",
      "Epoch: 110 loss_train: 1.1621 acc_train: 0.6786 loss_val: 1.1293 acc_val: 0.7030 time: 0.0031s\n",
      "Epoch: 111 loss_train: 1.1638 acc_train: 0.6595 loss_val: 1.1229 acc_val: 0.7048 time: 0.0032s\n",
      "Epoch: 112 loss_train: 1.1597 acc_train: 0.6613 loss_val: 1.1164 acc_val: 0.7066 time: 0.0031s\n",
      "Epoch: 113 loss_train: 1.1570 acc_train: 0.6718 loss_val: 1.1099 acc_val: 0.7103 time: 0.0031s\n",
      "Epoch: 114 loss_train: 1.1446 acc_train: 0.6755 loss_val: 1.1036 acc_val: 0.7140 time: 0.0034s\n",
      "Epoch: 115 loss_train: 1.1382 acc_train: 0.6755 loss_val: 1.0974 acc_val: 0.7159 time: 0.0035s\n",
      "Epoch: 116 loss_train: 1.1238 acc_train: 0.6853 loss_val: 1.0912 acc_val: 0.7159 time: 0.0032s\n",
      "Epoch: 117 loss_train: 1.1375 acc_train: 0.6656 loss_val: 1.0851 acc_val: 0.7159 time: 0.0033s\n",
      "Epoch: 118 loss_train: 1.1132 acc_train: 0.6872 loss_val: 1.0789 acc_val: 0.7159 time: 0.0040s\n",
      "Epoch: 119 loss_train: 1.1162 acc_train: 0.6847 loss_val: 1.0730 acc_val: 0.7177 time: 0.0037s\n",
      "Epoch: 120 loss_train: 1.1104 acc_train: 0.6872 loss_val: 1.0673 acc_val: 0.7196 time: 0.0032s\n",
      "Epoch: 121 loss_train: 1.1067 acc_train: 0.6817 loss_val: 1.0618 acc_val: 0.7196 time: 0.0031s\n",
      "Epoch: 122 loss_train: 1.1054 acc_train: 0.6823 loss_val: 1.0565 acc_val: 0.7196 time: 0.0033s\n",
      "Epoch: 123 loss_train: 1.0986 acc_train: 0.6909 loss_val: 1.0512 acc_val: 0.7214 time: 0.0032s\n",
      "Epoch: 124 loss_train: 1.0934 acc_train: 0.6903 loss_val: 1.0460 acc_val: 0.7251 time: 0.0032s\n",
      "Epoch: 125 loss_train: 1.0755 acc_train: 0.6897 loss_val: 1.0410 acc_val: 0.7251 time: 0.0033s\n",
      "Epoch: 126 loss_train: 1.0737 acc_train: 0.6995 loss_val: 1.0358 acc_val: 0.7325 time: 0.0033s\n",
      "Epoch: 127 loss_train: 1.0766 acc_train: 0.7100 loss_val: 1.0307 acc_val: 0.7343 time: 0.0032s\n",
      "Epoch: 128 loss_train: 1.0682 acc_train: 0.6946 loss_val: 1.0256 acc_val: 0.7417 time: 0.0035s\n",
      "Epoch: 129 loss_train: 1.0710 acc_train: 0.6989 loss_val: 1.0208 acc_val: 0.7491 time: 0.0036s\n",
      "Epoch: 130 loss_train: 1.0602 acc_train: 0.7014 loss_val: 1.0159 acc_val: 0.7509 time: 0.0034s\n",
      "Epoch: 131 loss_train: 1.0598 acc_train: 0.7106 loss_val: 1.0107 acc_val: 0.7528 time: 0.0033s\n",
      "Epoch: 132 loss_train: 1.0562 acc_train: 0.7020 loss_val: 1.0056 acc_val: 0.7546 time: 0.0031s\n",
      "Epoch: 133 loss_train: 1.0541 acc_train: 0.7069 loss_val: 1.0006 acc_val: 0.7620 time: 0.0031s\n",
      "Epoch: 134 loss_train: 1.0341 acc_train: 0.7180 loss_val: 0.9956 acc_val: 0.7638 time: 0.0032s\n",
      "Epoch: 135 loss_train: 1.0437 acc_train: 0.7057 loss_val: 0.9909 acc_val: 0.7657 time: 0.0031s\n",
      "Epoch: 136 loss_train: 1.0420 acc_train: 0.7211 loss_val: 0.9863 acc_val: 0.7657 time: 0.0033s\n",
      "Epoch: 137 loss_train: 1.0370 acc_train: 0.7167 loss_val: 0.9819 acc_val: 0.7638 time: 0.0034s\n",
      "Epoch: 138 loss_train: 1.0269 acc_train: 0.7143 loss_val: 0.9776 acc_val: 0.7638 time: 0.0034s\n",
      "Epoch: 139 loss_train: 1.0187 acc_train: 0.7180 loss_val: 0.9735 acc_val: 0.7638 time: 0.0031s\n",
      "Epoch: 140 loss_train: 1.0177 acc_train: 0.7248 loss_val: 0.9692 acc_val: 0.7620 time: 0.0031s\n",
      "Epoch: 141 loss_train: 1.0094 acc_train: 0.7371 loss_val: 0.9647 acc_val: 0.7675 time: 0.0031s\n",
      "Epoch: 142 loss_train: 0.9960 acc_train: 0.7315 loss_val: 0.9602 acc_val: 0.7675 time: 0.0031s\n",
      "Epoch: 143 loss_train: 0.9907 acc_train: 0.7420 loss_val: 0.9556 acc_val: 0.7694 time: 0.0032s\n",
      "Epoch: 144 loss_train: 1.0071 acc_train: 0.7346 loss_val: 0.9512 acc_val: 0.7731 time: 0.0035s\n",
      "Epoch: 145 loss_train: 0.9837 acc_train: 0.7297 loss_val: 0.9469 acc_val: 0.7731 time: 0.0034s\n",
      "Epoch: 146 loss_train: 0.9939 acc_train: 0.7278 loss_val: 0.9426 acc_val: 0.7768 time: 0.0034s\n",
      "Epoch: 147 loss_train: 0.9743 acc_train: 0.7401 loss_val: 0.9386 acc_val: 0.7768 time: 0.0032s\n",
      "Epoch: 148 loss_train: 0.9787 acc_train: 0.7328 loss_val: 0.9347 acc_val: 0.7749 time: 0.0033s\n",
      "Epoch: 149 loss_train: 0.9777 acc_train: 0.7371 loss_val: 0.9309 acc_val: 0.7768 time: 0.0033s\n",
      "Epoch: 150 loss_train: 0.9777 acc_train: 0.7463 loss_val: 0.9271 acc_val: 0.7768 time: 0.0031s\n",
      "Epoch: 151 loss_train: 0.9600 acc_train: 0.7586 loss_val: 0.9234 acc_val: 0.7768 time: 0.0031s\n",
      "Epoch: 152 loss_train: 0.9546 acc_train: 0.7512 loss_val: 0.9195 acc_val: 0.7768 time: 0.0031s\n",
      "Epoch: 153 loss_train: 0.9541 acc_train: 0.7525 loss_val: 0.9156 acc_val: 0.7768 time: 0.0031s\n",
      "Epoch: 154 loss_train: 0.9585 acc_train: 0.7562 loss_val: 0.9118 acc_val: 0.7768 time: 0.0033s\n",
      "Epoch: 155 loss_train: 0.9660 acc_train: 0.7457 loss_val: 0.9080 acc_val: 0.7804 time: 0.0031s\n",
      "Epoch: 156 loss_train: 0.9579 acc_train: 0.7401 loss_val: 0.9046 acc_val: 0.7786 time: 0.0031s\n",
      "Epoch: 157 loss_train: 0.9501 acc_train: 0.7512 loss_val: 0.9014 acc_val: 0.7841 time: 0.0034s\n",
      "Epoch: 158 loss_train: 0.9317 acc_train: 0.7629 loss_val: 0.8982 acc_val: 0.7860 time: 0.0034s\n",
      "Epoch: 159 loss_train: 0.9429 acc_train: 0.7432 loss_val: 0.8948 acc_val: 0.7915 time: 0.0031s\n",
      "Epoch: 160 loss_train: 0.9468 acc_train: 0.7420 loss_val: 0.8915 acc_val: 0.7952 time: 0.0034s\n",
      "Epoch: 161 loss_train: 0.9276 acc_train: 0.7586 loss_val: 0.8881 acc_val: 0.7970 time: 0.0031s\n",
      "Epoch: 162 loss_train: 0.9270 acc_train: 0.7642 loss_val: 0.8846 acc_val: 0.7952 time: 0.0032s\n",
      "Epoch: 163 loss_train: 0.9266 acc_train: 0.7635 loss_val: 0.8812 acc_val: 0.8007 time: 0.0031s\n",
      "Epoch: 164 loss_train: 0.9227 acc_train: 0.7709 loss_val: 0.8780 acc_val: 0.8007 time: 0.0033s\n",
      "Epoch: 165 loss_train: 0.8970 acc_train: 0.7789 loss_val: 0.8745 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 166 loss_train: 0.9055 acc_train: 0.7592 loss_val: 0.8709 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 167 loss_train: 0.9155 acc_train: 0.7679 loss_val: 0.8674 acc_val: 0.8044 time: 0.0031s\n",
      "Epoch: 168 loss_train: 0.8963 acc_train: 0.7796 loss_val: 0.8639 acc_val: 0.8044 time: 0.0031s\n",
      "Epoch: 169 loss_train: 0.9190 acc_train: 0.7635 loss_val: 0.8606 acc_val: 0.8063 time: 0.0032s\n",
      "Epoch: 170 loss_train: 0.9103 acc_train: 0.7666 loss_val: 0.8577 acc_val: 0.8118 time: 0.0033s\n",
      "Epoch: 171 loss_train: 0.9032 acc_train: 0.7709 loss_val: 0.8550 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 172 loss_train: 0.9025 acc_train: 0.7586 loss_val: 0.8526 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 173 loss_train: 0.8885 acc_train: 0.7685 loss_val: 0.8501 acc_val: 0.8137 time: 0.0034s\n",
      "Epoch: 174 loss_train: 0.9050 acc_train: 0.7666 loss_val: 0.8477 acc_val: 0.8155 time: 0.0034s\n",
      "Epoch: 175 loss_train: 0.8861 acc_train: 0.7894 loss_val: 0.8452 acc_val: 0.8210 time: 0.0031s\n",
      "Epoch: 176 loss_train: 0.8948 acc_train: 0.7728 loss_val: 0.8428 acc_val: 0.8210 time: 0.0032s\n",
      "Epoch: 177 loss_train: 0.8911 acc_train: 0.7746 loss_val: 0.8403 acc_val: 0.8210 time: 0.0031s\n",
      "Epoch: 178 loss_train: 0.8878 acc_train: 0.7746 loss_val: 0.8378 acc_val: 0.8229 time: 0.0031s\n",
      "Epoch: 179 loss_train: 0.8963 acc_train: 0.7709 loss_val: 0.8358 acc_val: 0.8192 time: 0.0031s\n",
      "Epoch: 180 loss_train: 0.8816 acc_train: 0.7882 loss_val: 0.8337 acc_val: 0.8192 time: 0.0042s\n",
      "Epoch: 181 loss_train: 0.8770 acc_train: 0.7808 loss_val: 0.8311 acc_val: 0.8210 time: 0.0035s\n",
      "Epoch: 182 loss_train: 0.8690 acc_train: 0.7900 loss_val: 0.8280 acc_val: 0.8229 time: 0.0033s\n",
      "Epoch: 183 loss_train: 0.8683 acc_train: 0.7814 loss_val: 0.8250 acc_val: 0.8266 time: 0.0032s\n",
      "Epoch: 184 loss_train: 0.8606 acc_train: 0.7869 loss_val: 0.8219 acc_val: 0.8284 time: 0.0033s\n",
      "Epoch: 185 loss_train: 0.8770 acc_train: 0.7783 loss_val: 0.8190 acc_val: 0.8303 time: 0.0034s\n",
      "Epoch: 186 loss_train: 0.8628 acc_train: 0.7820 loss_val: 0.8164 acc_val: 0.8339 time: 0.0032s\n",
      "Epoch: 187 loss_train: 0.8594 acc_train: 0.7888 loss_val: 0.8140 acc_val: 0.8339 time: 0.0033s\n",
      "Epoch: 188 loss_train: 0.8605 acc_train: 0.7796 loss_val: 0.8120 acc_val: 0.8339 time: 0.0033s\n",
      "Epoch: 189 loss_train: 0.8491 acc_train: 0.7999 loss_val: 0.8104 acc_val: 0.8339 time: 0.0031s\n",
      "Epoch: 190 loss_train: 0.8553 acc_train: 0.7796 loss_val: 0.8087 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 191 loss_train: 0.8531 acc_train: 0.7882 loss_val: 0.8069 acc_val: 0.8339 time: 0.0032s\n",
      "Epoch: 192 loss_train: 0.8499 acc_train: 0.7937 loss_val: 0.8047 acc_val: 0.8376 time: 0.0032s\n",
      "Epoch: 193 loss_train: 0.8537 acc_train: 0.7863 loss_val: 0.8023 acc_val: 0.8339 time: 0.0032s\n",
      "Epoch: 194 loss_train: 0.8564 acc_train: 0.7950 loss_val: 0.8002 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 195 loss_train: 0.8425 acc_train: 0.7913 loss_val: 0.7979 acc_val: 0.8395 time: 0.0031s\n",
      "Epoch: 196 loss_train: 0.8432 acc_train: 0.7820 loss_val: 0.7955 acc_val: 0.8395 time: 0.0034s\n",
      "Epoch: 197 loss_train: 0.8426 acc_train: 0.7943 loss_val: 0.7934 acc_val: 0.8358 time: 0.0034s\n",
      "Epoch: 198 loss_train: 0.8327 acc_train: 0.8017 loss_val: 0.7910 acc_val: 0.8358 time: 0.0033s\n",
      "Epoch: 199 loss_train: 0.8317 acc_train: 0.8110 loss_val: 0.7885 acc_val: 0.8358 time: 0.0033s\n",
      "Epoch: 200 loss_train: 0.8306 acc_train: 0.7968 loss_val: 0.7860 acc_val: 0.8358 time: 0.0031s\n",
      "Test set results: loss= 0.8219 accuracy= 0.8044\n",
      "Training the model for the 2th time\n",
      "Epoch: 001 loss_train: 1.9585 acc_train: 0.1312 loss_val: 1.9544 acc_val: 0.1255 time: 0.0032s\n",
      "Epoch: 002 loss_train: 1.9527 acc_train: 0.1312 loss_val: 1.9489 acc_val: 0.1273 time: 0.0031s\n",
      "Epoch: 003 loss_train: 1.9472 acc_train: 0.1324 loss_val: 1.9433 acc_val: 0.1273 time: 0.0031s\n",
      "Epoch: 004 loss_train: 1.9419 acc_train: 0.1324 loss_val: 1.9378 acc_val: 0.1328 time: 0.0031s\n",
      "Epoch: 005 loss_train: 1.9360 acc_train: 0.1626 loss_val: 1.9320 acc_val: 0.1919 time: 0.0031s\n",
      "Epoch: 006 loss_train: 1.9307 acc_train: 0.1835 loss_val: 1.9259 acc_val: 0.2528 time: 0.0034s\n",
      "Epoch: 007 loss_train: 1.9246 acc_train: 0.2174 loss_val: 1.9194 acc_val: 0.2399 time: 0.0032s\n",
      "Epoch: 008 loss_train: 1.9188 acc_train: 0.2500 loss_val: 1.9125 acc_val: 0.3229 time: 0.0031s\n",
      "Epoch: 009 loss_train: 1.9116 acc_train: 0.2888 loss_val: 1.9054 acc_val: 0.4428 time: 0.0031s\n",
      "Epoch: 010 loss_train: 1.9047 acc_train: 0.3208 loss_val: 1.8981 acc_val: 0.4613 time: 0.0031s\n",
      "Epoch: 011 loss_train: 1.8993 acc_train: 0.3393 loss_val: 1.8906 acc_val: 0.4059 time: 0.0031s\n",
      "Epoch: 012 loss_train: 1.8915 acc_train: 0.3381 loss_val: 1.8829 acc_val: 0.3542 time: 0.0031s\n",
      "Epoch: 013 loss_train: 1.8832 acc_train: 0.3239 loss_val: 1.8751 acc_val: 0.3321 time: 0.0032s\n",
      "Epoch: 014 loss_train: 1.8747 acc_train: 0.3300 loss_val: 1.8672 acc_val: 0.3229 time: 0.0031s\n",
      "Epoch: 015 loss_train: 1.8679 acc_train: 0.3227 loss_val: 1.8592 acc_val: 0.3155 time: 0.0034s\n",
      "Epoch: 016 loss_train: 1.8602 acc_train: 0.2974 loss_val: 1.8513 acc_val: 0.3137 time: 0.0034s\n",
      "Epoch: 017 loss_train: 1.8512 acc_train: 0.3177 loss_val: 1.8433 acc_val: 0.3137 time: 0.0035s\n",
      "Epoch: 018 loss_train: 1.8445 acc_train: 0.3134 loss_val: 1.8355 acc_val: 0.3137 time: 0.0032s\n",
      "Epoch: 019 loss_train: 1.8373 acc_train: 0.3134 loss_val: 1.8278 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 020 loss_train: 1.8316 acc_train: 0.3134 loss_val: 1.8203 acc_val: 0.3137 time: 0.0032s\n",
      "Epoch: 021 loss_train: 1.8223 acc_train: 0.2962 loss_val: 1.8130 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 022 loss_train: 1.8137 acc_train: 0.3011 loss_val: 1.8060 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 023 loss_train: 1.8098 acc_train: 0.2980 loss_val: 1.7993 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 024 loss_train: 1.8029 acc_train: 0.2919 loss_val: 1.7928 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 025 loss_train: 1.7930 acc_train: 0.2956 loss_val: 1.7865 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 026 loss_train: 1.7882 acc_train: 0.2980 loss_val: 1.7805 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 027 loss_train: 1.7813 acc_train: 0.2980 loss_val: 1.7745 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 028 loss_train: 1.7807 acc_train: 0.2980 loss_val: 1.7688 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 029 loss_train: 1.7664 acc_train: 0.2974 loss_val: 1.7630 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 030 loss_train: 1.7658 acc_train: 0.2968 loss_val: 1.7573 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 031 loss_train: 1.7632 acc_train: 0.2986 loss_val: 1.7515 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 032 loss_train: 1.7572 acc_train: 0.2980 loss_val: 1.7456 acc_val: 0.3137 time: 0.0032s\n",
      "Epoch: 033 loss_train: 1.7479 acc_train: 0.2980 loss_val: 1.7394 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 034 loss_train: 1.7419 acc_train: 0.2999 loss_val: 1.7332 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 035 loss_train: 1.7373 acc_train: 0.2993 loss_val: 1.7267 acc_val: 0.3137 time: 0.0034s\n",
      "Epoch: 036 loss_train: 1.7278 acc_train: 0.3030 loss_val: 1.7200 acc_val: 0.3137 time: 0.0034s\n",
      "Epoch: 037 loss_train: 1.7238 acc_train: 0.3067 loss_val: 1.7132 acc_val: 0.3137 time: 0.0032s\n",
      "Epoch: 038 loss_train: 1.7224 acc_train: 0.3073 loss_val: 1.7063 acc_val: 0.3137 time: 0.0034s\n",
      "Epoch: 039 loss_train: 1.7138 acc_train: 0.3122 loss_val: 1.6991 acc_val: 0.3137 time: 0.0033s\n",
      "Epoch: 040 loss_train: 1.7082 acc_train: 0.3110 loss_val: 1.6919 acc_val: 0.3137 time: 0.0034s\n",
      "Epoch: 041 loss_train: 1.7008 acc_train: 0.3177 loss_val: 1.6846 acc_val: 0.3137 time: 0.0039s\n",
      "Epoch: 042 loss_train: 1.6937 acc_train: 0.3177 loss_val: 1.6773 acc_val: 0.3229 time: 0.0036s\n",
      "Epoch: 043 loss_train: 1.6791 acc_train: 0.3282 loss_val: 1.6698 acc_val: 0.3247 time: 0.0031s\n",
      "Epoch: 044 loss_train: 1.6777 acc_train: 0.3442 loss_val: 1.6623 acc_val: 0.3284 time: 0.0032s\n",
      "Epoch: 045 loss_train: 1.6688 acc_train: 0.3578 loss_val: 1.6547 acc_val: 0.3413 time: 0.0033s\n",
      "Epoch: 046 loss_train: 1.6611 acc_train: 0.3504 loss_val: 1.6469 acc_val: 0.3469 time: 0.0035s\n",
      "Epoch: 047 loss_train: 1.6565 acc_train: 0.3615 loss_val: 1.6391 acc_val: 0.3616 time: 0.0032s\n",
      "Epoch: 048 loss_train: 1.6452 acc_train: 0.3812 loss_val: 1.6311 acc_val: 0.3708 time: 0.0032s\n",
      "Epoch: 049 loss_train: 1.6426 acc_train: 0.3768 loss_val: 1.6229 acc_val: 0.3764 time: 0.0034s\n",
      "Epoch: 050 loss_train: 1.6330 acc_train: 0.3849 loss_val: 1.6147 acc_val: 0.3856 time: 0.0034s\n",
      "Epoch: 051 loss_train: 1.6234 acc_train: 0.3941 loss_val: 1.6062 acc_val: 0.3948 time: 0.0035s\n",
      "Epoch: 052 loss_train: 1.6099 acc_train: 0.4027 loss_val: 1.5976 acc_val: 0.4004 time: 0.0031s\n",
      "Epoch: 053 loss_train: 1.6011 acc_train: 0.4083 loss_val: 1.5889 acc_val: 0.4022 time: 0.0031s\n",
      "Epoch: 054 loss_train: 1.5916 acc_train: 0.4187 loss_val: 1.5800 acc_val: 0.4096 time: 0.0031s\n",
      "Epoch: 055 loss_train: 1.5865 acc_train: 0.4200 loss_val: 1.5710 acc_val: 0.4151 time: 0.0031s\n",
      "Epoch: 056 loss_train: 1.5813 acc_train: 0.4200 loss_val: 1.5620 acc_val: 0.4170 time: 0.0031s\n",
      "Epoch: 057 loss_train: 1.5738 acc_train: 0.4335 loss_val: 1.5529 acc_val: 0.4262 time: 0.0031s\n",
      "Epoch: 058 loss_train: 1.5635 acc_train: 0.4236 loss_val: 1.5437 acc_val: 0.4354 time: 0.0034s\n",
      "Epoch: 059 loss_train: 1.5465 acc_train: 0.4347 loss_val: 1.5344 acc_val: 0.4428 time: 0.0034s\n",
      "Epoch: 060 loss_train: 1.5484 acc_train: 0.4292 loss_val: 1.5252 acc_val: 0.4446 time: 0.0034s\n",
      "Epoch: 061 loss_train: 1.5317 acc_train: 0.4415 loss_val: 1.5159 acc_val: 0.4520 time: 0.0034s\n",
      "Epoch: 062 loss_train: 1.5234 acc_train: 0.4353 loss_val: 1.5066 acc_val: 0.4557 time: 0.0032s\n",
      "Epoch: 063 loss_train: 1.5200 acc_train: 0.4532 loss_val: 1.4973 acc_val: 0.4631 time: 0.0031s\n",
      "Epoch: 064 loss_train: 1.5042 acc_train: 0.4569 loss_val: 1.4880 acc_val: 0.4668 time: 0.0031s\n",
      "Epoch: 065 loss_train: 1.4964 acc_train: 0.4618 loss_val: 1.4788 acc_val: 0.4686 time: 0.0031s\n",
      "Epoch: 066 loss_train: 1.4888 acc_train: 0.4581 loss_val: 1.4696 acc_val: 0.4742 time: 0.0031s\n",
      "Epoch: 067 loss_train: 1.4809 acc_train: 0.4766 loss_val: 1.4604 acc_val: 0.4797 time: 0.0032s\n",
      "Epoch: 068 loss_train: 1.4596 acc_train: 0.4784 loss_val: 1.4512 acc_val: 0.4815 time: 0.0034s\n",
      "Epoch: 069 loss_train: 1.4578 acc_train: 0.4815 loss_val: 1.4420 acc_val: 0.4871 time: 0.0034s\n",
      "Epoch: 070 loss_train: 1.4497 acc_train: 0.4834 loss_val: 1.4329 acc_val: 0.4908 time: 0.0034s\n",
      "Epoch: 071 loss_train: 1.4375 acc_train: 0.4858 loss_val: 1.4238 acc_val: 0.4926 time: 0.0031s\n",
      "Epoch: 072 loss_train: 1.4380 acc_train: 0.4914 loss_val: 1.4148 acc_val: 0.4945 time: 0.0031s\n",
      "Epoch: 073 loss_train: 1.4217 acc_train: 0.4975 loss_val: 1.4058 acc_val: 0.5037 time: 0.0031s\n",
      "Epoch: 074 loss_train: 1.4115 acc_train: 0.5129 loss_val: 1.3969 acc_val: 0.5092 time: 0.0031s\n",
      "Epoch: 075 loss_train: 1.4042 acc_train: 0.5086 loss_val: 1.3880 acc_val: 0.5129 time: 0.0031s\n",
      "Epoch: 076 loss_train: 1.3958 acc_train: 0.5086 loss_val: 1.3791 acc_val: 0.5203 time: 0.0034s\n",
      "Epoch: 077 loss_train: 1.3961 acc_train: 0.5197 loss_val: 1.3703 acc_val: 0.5221 time: 0.0032s\n",
      "Epoch: 078 loss_train: 1.3866 acc_train: 0.5166 loss_val: 1.3616 acc_val: 0.5258 time: 0.0036s\n",
      "Epoch: 079 loss_train: 1.3707 acc_train: 0.5339 loss_val: 1.3529 acc_val: 0.5314 time: 0.0035s\n",
      "Epoch: 080 loss_train: 1.3650 acc_train: 0.5259 loss_val: 1.3443 acc_val: 0.5369 time: 0.0034s\n",
      "Epoch: 081 loss_train: 1.3528 acc_train: 0.5388 loss_val: 1.3358 acc_val: 0.5406 time: 0.0032s\n",
      "Epoch: 082 loss_train: 1.3571 acc_train: 0.5480 loss_val: 1.3273 acc_val: 0.5424 time: 0.0032s\n",
      "Epoch: 083 loss_train: 1.3374 acc_train: 0.5450 loss_val: 1.3190 acc_val: 0.5443 time: 0.0032s\n",
      "Epoch: 084 loss_train: 1.3234 acc_train: 0.5647 loss_val: 1.3108 acc_val: 0.5443 time: 0.0031s\n",
      "Epoch: 085 loss_train: 1.3140 acc_train: 0.5616 loss_val: 1.3026 acc_val: 0.5461 time: 0.0033s\n",
      "Epoch: 086 loss_train: 1.3110 acc_train: 0.5653 loss_val: 1.2944 acc_val: 0.5498 time: 0.0033s\n",
      "Epoch: 087 loss_train: 1.3054 acc_train: 0.5585 loss_val: 1.2864 acc_val: 0.5517 time: 0.0032s\n",
      "Epoch: 088 loss_train: 1.3041 acc_train: 0.5616 loss_val: 1.2784 acc_val: 0.5572 time: 0.0034s\n",
      "Epoch: 089 loss_train: 1.2857 acc_train: 0.5671 loss_val: 1.2704 acc_val: 0.5646 time: 0.0036s\n",
      "Epoch: 090 loss_train: 1.2889 acc_train: 0.5683 loss_val: 1.2626 acc_val: 0.5830 time: 0.0035s\n",
      "Epoch: 091 loss_train: 1.2865 acc_train: 0.5819 loss_val: 1.2549 acc_val: 0.6015 time: 0.0034s\n",
      "Epoch: 092 loss_train: 1.2628 acc_train: 0.5998 loss_val: 1.2472 acc_val: 0.6107 time: 0.0032s\n",
      "Epoch: 093 loss_train: 1.2558 acc_train: 0.5954 loss_val: 1.2396 acc_val: 0.6162 time: 0.0031s\n",
      "Epoch: 094 loss_train: 1.2666 acc_train: 0.5899 loss_val: 1.2321 acc_val: 0.6199 time: 0.0034s\n",
      "Epoch: 095 loss_train: 1.2544 acc_train: 0.6065 loss_val: 1.2248 acc_val: 0.6218 time: 0.0033s\n",
      "Epoch: 096 loss_train: 1.2494 acc_train: 0.6065 loss_val: 1.2176 acc_val: 0.6310 time: 0.0032s\n",
      "Epoch: 097 loss_train: 1.2363 acc_train: 0.6133 loss_val: 1.2103 acc_val: 0.6384 time: 0.0032s\n",
      "Epoch: 098 loss_train: 1.2264 acc_train: 0.6096 loss_val: 1.2031 acc_val: 0.6384 time: 0.0033s\n",
      "Epoch: 099 loss_train: 1.2221 acc_train: 0.6022 loss_val: 1.1959 acc_val: 0.6458 time: 0.0033s\n",
      "Epoch: 100 loss_train: 1.2228 acc_train: 0.6188 loss_val: 1.1889 acc_val: 0.6550 time: 0.0032s\n",
      "Epoch: 101 loss_train: 1.1892 acc_train: 0.6392 loss_val: 1.1820 acc_val: 0.6605 time: 0.0031s\n",
      "Epoch: 102 loss_train: 1.2017 acc_train: 0.6145 loss_val: 1.1752 acc_val: 0.6642 time: 0.0035s\n",
      "Epoch: 103 loss_train: 1.1960 acc_train: 0.6373 loss_val: 1.1684 acc_val: 0.6679 time: 0.0037s\n",
      "Epoch: 104 loss_train: 1.1960 acc_train: 0.6305 loss_val: 1.1615 acc_val: 0.6734 time: 0.0032s\n",
      "Epoch: 105 loss_train: 1.1864 acc_train: 0.6355 loss_val: 1.1546 acc_val: 0.6771 time: 0.0035s\n",
      "Epoch: 106 loss_train: 1.1763 acc_train: 0.6373 loss_val: 1.1478 acc_val: 0.6790 time: 0.0033s\n",
      "Epoch: 107 loss_train: 1.1727 acc_train: 0.6613 loss_val: 1.1410 acc_val: 0.6827 time: 0.0034s\n",
      "Epoch: 108 loss_train: 1.1677 acc_train: 0.6736 loss_val: 1.1343 acc_val: 0.6845 time: 0.0037s\n",
      "Epoch: 109 loss_train: 1.1648 acc_train: 0.6583 loss_val: 1.1276 acc_val: 0.6863 time: 0.0034s\n",
      "Epoch: 110 loss_train: 1.1507 acc_train: 0.6866 loss_val: 1.1209 acc_val: 0.6900 time: 0.0033s\n",
      "Epoch: 111 loss_train: 1.1481 acc_train: 0.6897 loss_val: 1.1142 acc_val: 0.6956 time: 0.0032s\n",
      "Epoch: 112 loss_train: 1.1344 acc_train: 0.6724 loss_val: 1.1075 acc_val: 0.6974 time: 0.0032s\n",
      "Epoch: 113 loss_train: 1.1377 acc_train: 0.6897 loss_val: 1.1008 acc_val: 0.6974 time: 0.0031s\n",
      "Epoch: 114 loss_train: 1.1297 acc_train: 0.6866 loss_val: 1.0941 acc_val: 0.7103 time: 0.0034s\n",
      "Epoch: 115 loss_train: 1.1147 acc_train: 0.6921 loss_val: 1.0875 acc_val: 0.7140 time: 0.0032s\n",
      "Epoch: 116 loss_train: 1.1260 acc_train: 0.6909 loss_val: 1.0811 acc_val: 0.7196 time: 0.0034s\n",
      "Epoch: 117 loss_train: 1.1143 acc_train: 0.6940 loss_val: 1.0748 acc_val: 0.7232 time: 0.0033s\n",
      "Epoch: 118 loss_train: 1.1039 acc_train: 0.6946 loss_val: 1.0687 acc_val: 0.7306 time: 0.0033s\n",
      "Epoch: 119 loss_train: 1.1159 acc_train: 0.7026 loss_val: 1.0628 acc_val: 0.7362 time: 0.0031s\n",
      "Epoch: 120 loss_train: 1.0880 acc_train: 0.7124 loss_val: 1.0571 acc_val: 0.7362 time: 0.0031s\n",
      "Epoch: 121 loss_train: 1.0859 acc_train: 0.7094 loss_val: 1.0517 acc_val: 0.7343 time: 0.0032s\n",
      "Epoch: 122 loss_train: 1.0801 acc_train: 0.7155 loss_val: 1.0461 acc_val: 0.7343 time: 0.0034s\n",
      "Epoch: 123 loss_train: 1.0810 acc_train: 0.7155 loss_val: 1.0406 acc_val: 0.7380 time: 0.0033s\n",
      "Epoch: 124 loss_train: 1.0795 acc_train: 0.7137 loss_val: 1.0352 acc_val: 0.7399 time: 0.0031s\n",
      "Epoch: 125 loss_train: 1.0629 acc_train: 0.7217 loss_val: 1.0295 acc_val: 0.7491 time: 0.0033s\n",
      "Epoch: 126 loss_train: 1.0600 acc_train: 0.7278 loss_val: 1.0238 acc_val: 0.7546 time: 0.0031s\n",
      "Epoch: 127 loss_train: 1.0463 acc_train: 0.7241 loss_val: 1.0182 acc_val: 0.7546 time: 0.0032s\n",
      "Epoch: 128 loss_train: 1.0451 acc_train: 0.7223 loss_val: 1.0125 acc_val: 0.7565 time: 0.0031s\n",
      "Epoch: 129 loss_train: 1.0373 acc_train: 0.7260 loss_val: 1.0069 acc_val: 0.7565 time: 0.0033s\n",
      "Epoch: 130 loss_train: 1.0408 acc_train: 0.7303 loss_val: 1.0013 acc_val: 0.7601 time: 0.0031s\n",
      "Epoch: 131 loss_train: 1.0307 acc_train: 0.7346 loss_val: 0.9959 acc_val: 0.7638 time: 0.0031s\n",
      "Epoch: 132 loss_train: 1.0288 acc_train: 0.7475 loss_val: 0.9906 acc_val: 0.7675 time: 0.0034s\n",
      "Epoch: 133 loss_train: 1.0349 acc_train: 0.7352 loss_val: 0.9856 acc_val: 0.7638 time: 0.0034s\n",
      "Epoch: 134 loss_train: 1.0203 acc_train: 0.7475 loss_val: 0.9808 acc_val: 0.7657 time: 0.0031s\n",
      "Epoch: 135 loss_train: 1.0115 acc_train: 0.7531 loss_val: 0.9760 acc_val: 0.7731 time: 0.0031s\n",
      "Epoch: 136 loss_train: 1.0064 acc_train: 0.7586 loss_val: 0.9710 acc_val: 0.7731 time: 0.0031s\n",
      "Epoch: 137 loss_train: 1.0041 acc_train: 0.7494 loss_val: 0.9660 acc_val: 0.7841 time: 0.0031s\n",
      "Epoch: 138 loss_train: 1.0008 acc_train: 0.7555 loss_val: 0.9611 acc_val: 0.7897 time: 0.0031s\n",
      "Epoch: 139 loss_train: 0.9931 acc_train: 0.7666 loss_val: 0.9562 acc_val: 0.7934 time: 0.0031s\n",
      "Epoch: 140 loss_train: 1.0047 acc_train: 0.7463 loss_val: 0.9514 acc_val: 0.7989 time: 0.0034s\n",
      "Epoch: 141 loss_train: 0.9965 acc_train: 0.7488 loss_val: 0.9467 acc_val: 0.8007 time: 0.0034s\n",
      "Epoch: 142 loss_train: 0.9857 acc_train: 0.7518 loss_val: 0.9421 acc_val: 0.8007 time: 0.0035s\n",
      "Epoch: 143 loss_train: 0.9823 acc_train: 0.7623 loss_val: 0.9377 acc_val: 0.8063 time: 0.0032s\n",
      "Epoch: 144 loss_train: 0.9848 acc_train: 0.7531 loss_val: 0.9332 acc_val: 0.8081 time: 0.0031s\n",
      "Epoch: 145 loss_train: 0.9716 acc_train: 0.7623 loss_val: 0.9287 acc_val: 0.8118 time: 0.0032s\n",
      "Epoch: 146 loss_train: 0.9661 acc_train: 0.7765 loss_val: 0.9241 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 147 loss_train: 0.9546 acc_train: 0.7716 loss_val: 0.9194 acc_val: 0.8155 time: 0.0031s\n",
      "Epoch: 148 loss_train: 0.9615 acc_train: 0.7746 loss_val: 0.9147 acc_val: 0.8173 time: 0.0031s\n",
      "Epoch: 149 loss_train: 0.9525 acc_train: 0.7709 loss_val: 0.9101 acc_val: 0.8192 time: 0.0034s\n",
      "Epoch: 150 loss_train: 0.9566 acc_train: 0.7679 loss_val: 0.9056 acc_val: 0.8155 time: 0.0033s\n",
      "Epoch: 151 loss_train: 0.9581 acc_train: 0.7518 loss_val: 0.9014 acc_val: 0.8155 time: 0.0032s\n",
      "Epoch: 152 loss_train: 0.9427 acc_train: 0.7814 loss_val: 0.8976 acc_val: 0.8155 time: 0.0032s\n",
      "Epoch: 153 loss_train: 0.9430 acc_train: 0.7635 loss_val: 0.8940 acc_val: 0.8155 time: 0.0031s\n",
      "Epoch: 154 loss_train: 0.9326 acc_train: 0.7740 loss_val: 0.8906 acc_val: 0.8192 time: 0.0032s\n",
      "Epoch: 155 loss_train: 0.9198 acc_train: 0.7765 loss_val: 0.8871 acc_val: 0.8210 time: 0.0031s\n",
      "Epoch: 156 loss_train: 0.9233 acc_train: 0.7796 loss_val: 0.8835 acc_val: 0.8210 time: 0.0031s\n",
      "Epoch: 157 loss_train: 0.9153 acc_train: 0.7839 loss_val: 0.8796 acc_val: 0.8173 time: 0.0031s\n",
      "Epoch: 158 loss_train: 0.9178 acc_train: 0.7789 loss_val: 0.8757 acc_val: 0.8173 time: 0.0032s\n",
      "Epoch: 159 loss_train: 0.9123 acc_train: 0.7808 loss_val: 0.8717 acc_val: 0.8192 time: 0.0034s\n",
      "Epoch: 160 loss_train: 0.9117 acc_train: 0.7882 loss_val: 0.8679 acc_val: 0.8192 time: 0.0034s\n",
      "Epoch: 161 loss_train: 0.9020 acc_train: 0.7863 loss_val: 0.8642 acc_val: 0.8192 time: 0.0034s\n",
      "Epoch: 162 loss_train: 0.9077 acc_train: 0.7808 loss_val: 0.8605 acc_val: 0.8192 time: 0.0031s\n",
      "Epoch: 163 loss_train: 0.9109 acc_train: 0.7808 loss_val: 0.8572 acc_val: 0.8210 time: 0.0031s\n",
      "Epoch: 164 loss_train: 0.8952 acc_train: 0.7925 loss_val: 0.8540 acc_val: 0.8210 time: 0.0032s\n",
      "Epoch: 165 loss_train: 0.9021 acc_train: 0.7783 loss_val: 0.8510 acc_val: 0.8210 time: 0.0034s\n",
      "Epoch: 166 loss_train: 0.8845 acc_train: 0.7986 loss_val: 0.8478 acc_val: 0.8210 time: 0.0032s\n",
      "Epoch: 167 loss_train: 0.8875 acc_train: 0.7937 loss_val: 0.8447 acc_val: 0.8210 time: 0.0032s\n",
      "Epoch: 168 loss_train: 0.8891 acc_train: 0.7913 loss_val: 0.8416 acc_val: 0.8229 time: 0.0033s\n",
      "Epoch: 169 loss_train: 0.8844 acc_train: 0.7857 loss_val: 0.8385 acc_val: 0.8229 time: 0.0033s\n",
      "Epoch: 170 loss_train: 0.8730 acc_train: 0.7894 loss_val: 0.8354 acc_val: 0.8303 time: 0.0033s\n",
      "Epoch: 171 loss_train: 0.8823 acc_train: 0.7894 loss_val: 0.8328 acc_val: 0.8303 time: 0.0031s\n",
      "Epoch: 172 loss_train: 0.8863 acc_train: 0.7980 loss_val: 0.8304 acc_val: 0.8284 time: 0.0034s\n",
      "Epoch: 173 loss_train: 0.8655 acc_train: 0.7974 loss_val: 0.8279 acc_val: 0.8284 time: 0.0034s\n",
      "Epoch: 174 loss_train: 0.8680 acc_train: 0.7993 loss_val: 0.8251 acc_val: 0.8266 time: 0.0036s\n",
      "Epoch: 175 loss_train: 0.8514 acc_train: 0.8048 loss_val: 0.8219 acc_val: 0.8266 time: 0.0034s\n",
      "Epoch: 176 loss_train: 0.8682 acc_train: 0.7894 loss_val: 0.8188 acc_val: 0.8303 time: 0.0033s\n",
      "Epoch: 177 loss_train: 0.8679 acc_train: 0.7882 loss_val: 0.8156 acc_val: 0.8303 time: 0.0032s\n",
      "Epoch: 178 loss_train: 0.8489 acc_train: 0.8036 loss_val: 0.8126 acc_val: 0.8303 time: 0.0032s\n",
      "Epoch: 179 loss_train: 0.8574 acc_train: 0.7980 loss_val: 0.8101 acc_val: 0.8284 time: 0.0033s\n",
      "Epoch: 180 loss_train: 0.8415 acc_train: 0.8067 loss_val: 0.8076 acc_val: 0.8284 time: 0.0033s\n",
      "Epoch: 181 loss_train: 0.8463 acc_train: 0.8005 loss_val: 0.8051 acc_val: 0.8266 time: 0.0031s\n",
      "Epoch: 182 loss_train: 0.8414 acc_train: 0.8042 loss_val: 0.8025 acc_val: 0.8284 time: 0.0031s\n",
      "Epoch: 183 loss_train: 0.8472 acc_train: 0.8011 loss_val: 0.7999 acc_val: 0.8266 time: 0.0032s\n",
      "Epoch: 184 loss_train: 0.8295 acc_train: 0.8030 loss_val: 0.7974 acc_val: 0.8303 time: 0.0031s\n",
      "Epoch: 185 loss_train: 0.8379 acc_train: 0.8011 loss_val: 0.7950 acc_val: 0.8303 time: 0.0031s\n",
      "Epoch: 186 loss_train: 0.8275 acc_train: 0.8134 loss_val: 0.7927 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 187 loss_train: 0.8407 acc_train: 0.7943 loss_val: 0.7905 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 188 loss_train: 0.8372 acc_train: 0.8079 loss_val: 0.7884 acc_val: 0.8321 time: 0.0034s\n",
      "Epoch: 189 loss_train: 0.8344 acc_train: 0.7888 loss_val: 0.7861 acc_val: 0.8284 time: 0.0034s\n",
      "Epoch: 190 loss_train: 0.8166 acc_train: 0.8116 loss_val: 0.7836 acc_val: 0.8321 time: 0.0033s\n",
      "Epoch: 191 loss_train: 0.8242 acc_train: 0.8140 loss_val: 0.7810 acc_val: 0.8321 time: 0.0033s\n",
      "Epoch: 192 loss_train: 0.8265 acc_train: 0.8011 loss_val: 0.7784 acc_val: 0.8339 time: 0.0031s\n",
      "Epoch: 193 loss_train: 0.8083 acc_train: 0.8122 loss_val: 0.7760 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 194 loss_train: 0.8051 acc_train: 0.8091 loss_val: 0.7735 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 195 loss_train: 0.8276 acc_train: 0.7986 loss_val: 0.7714 acc_val: 0.8321 time: 0.0033s\n",
      "Epoch: 196 loss_train: 0.8124 acc_train: 0.8183 loss_val: 0.7693 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 197 loss_train: 0.8261 acc_train: 0.7950 loss_val: 0.7673 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 198 loss_train: 0.8087 acc_train: 0.8116 loss_val: 0.7653 acc_val: 0.8339 time: 0.0031s\n",
      "Epoch: 199 loss_train: 0.8038 acc_train: 0.8017 loss_val: 0.7635 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 200 loss_train: 0.7875 acc_train: 0.8110 loss_val: 0.7616 acc_val: 0.8413 time: 0.0032s\n",
      "Test set results: loss= 0.7905 accuracy= 0.8321\n",
      "Training the model for the 3th time\n",
      "Epoch: 001 loss_train: 1.9364 acc_train: 0.1724 loss_val: 1.9335 acc_val: 0.1513 time: 0.0033s\n",
      "Epoch: 002 loss_train: 1.9312 acc_train: 0.1995 loss_val: 1.9282 acc_val: 0.2048 time: 0.0031s\n",
      "Epoch: 003 loss_train: 1.9256 acc_train: 0.2494 loss_val: 1.9230 acc_val: 0.3007 time: 0.0033s\n",
      "Epoch: 004 loss_train: 1.9200 acc_train: 0.2863 loss_val: 1.9174 acc_val: 0.3469 time: 0.0031s\n",
      "Epoch: 005 loss_train: 1.9146 acc_train: 0.2980 loss_val: 1.9114 acc_val: 0.3303 time: 0.0031s\n",
      "Epoch: 006 loss_train: 1.9087 acc_train: 0.2962 loss_val: 1.9051 acc_val: 0.3229 time: 0.0032s\n",
      "Epoch: 007 loss_train: 1.9021 acc_train: 0.3011 loss_val: 1.8986 acc_val: 0.3229 time: 0.0032s\n",
      "Epoch: 008 loss_train: 1.8967 acc_train: 0.2925 loss_val: 1.8919 acc_val: 0.3155 time: 0.0034s\n",
      "Epoch: 009 loss_train: 1.8901 acc_train: 0.3079 loss_val: 1.8850 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 010 loss_train: 1.8824 acc_train: 0.3085 loss_val: 1.8779 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 011 loss_train: 1.8761 acc_train: 0.3060 loss_val: 1.8708 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 012 loss_train: 1.8692 acc_train: 0.3091 loss_val: 1.8635 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 013 loss_train: 1.8613 acc_train: 0.3110 loss_val: 1.8563 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 014 loss_train: 1.8559 acc_train: 0.3085 loss_val: 1.8490 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 015 loss_train: 1.8475 acc_train: 0.3011 loss_val: 1.8417 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 016 loss_train: 1.8410 acc_train: 0.3017 loss_val: 1.8344 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 017 loss_train: 1.8316 acc_train: 0.3042 loss_val: 1.8272 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 018 loss_train: 1.8275 acc_train: 0.2962 loss_val: 1.8202 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 019 loss_train: 1.8203 acc_train: 0.2986 loss_val: 1.8132 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 020 loss_train: 1.8124 acc_train: 0.2993 loss_val: 1.8064 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 021 loss_train: 1.8077 acc_train: 0.2950 loss_val: 1.7999 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 022 loss_train: 1.8024 acc_train: 0.2993 loss_val: 1.7934 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 023 loss_train: 1.7980 acc_train: 0.2950 loss_val: 1.7872 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 024 loss_train: 1.7862 acc_train: 0.2956 loss_val: 1.7810 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 025 loss_train: 1.7873 acc_train: 0.2962 loss_val: 1.7750 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 026 loss_train: 1.7818 acc_train: 0.2993 loss_val: 1.7692 acc_val: 0.3118 time: 0.0037s\n",
      "Epoch: 027 loss_train: 1.7677 acc_train: 0.2956 loss_val: 1.7633 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 028 loss_train: 1.7683 acc_train: 0.2986 loss_val: 1.7575 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 029 loss_train: 1.7572 acc_train: 0.2986 loss_val: 1.7516 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 030 loss_train: 1.7563 acc_train: 0.2968 loss_val: 1.7457 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 031 loss_train: 1.7427 acc_train: 0.2956 loss_val: 1.7395 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 032 loss_train: 1.7435 acc_train: 0.2956 loss_val: 1.7333 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 033 loss_train: 1.7398 acc_train: 0.2968 loss_val: 1.7268 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 034 loss_train: 1.7328 acc_train: 0.2956 loss_val: 1.7201 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 035 loss_train: 1.7224 acc_train: 0.2993 loss_val: 1.7131 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 036 loss_train: 1.7129 acc_train: 0.3011 loss_val: 1.7060 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 037 loss_train: 1.7147 acc_train: 0.3023 loss_val: 1.6987 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 038 loss_train: 1.7042 acc_train: 0.3017 loss_val: 1.6912 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 039 loss_train: 1.6899 acc_train: 0.3042 loss_val: 1.6836 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 040 loss_train: 1.6932 acc_train: 0.3060 loss_val: 1.6758 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 041 loss_train: 1.6809 acc_train: 0.3103 loss_val: 1.6678 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 042 loss_train: 1.6720 acc_train: 0.3116 loss_val: 1.6597 acc_val: 0.3137 time: 0.0032s\n",
      "Epoch: 043 loss_train: 1.6626 acc_train: 0.3128 loss_val: 1.6515 acc_val: 0.3155 time: 0.0031s\n",
      "Epoch: 044 loss_train: 1.6567 acc_train: 0.3196 loss_val: 1.6431 acc_val: 0.3155 time: 0.0034s\n",
      "Epoch: 045 loss_train: 1.6457 acc_train: 0.3239 loss_val: 1.6346 acc_val: 0.3173 time: 0.0035s\n",
      "Epoch: 046 loss_train: 1.6427 acc_train: 0.3374 loss_val: 1.6260 acc_val: 0.3192 time: 0.0031s\n",
      "Epoch: 047 loss_train: 1.6274 acc_train: 0.3424 loss_val: 1.6173 acc_val: 0.3229 time: 0.0031s\n",
      "Epoch: 048 loss_train: 1.6232 acc_train: 0.3461 loss_val: 1.6085 acc_val: 0.3229 time: 0.0031s\n",
      "Epoch: 049 loss_train: 1.6131 acc_train: 0.3553 loss_val: 1.5996 acc_val: 0.3339 time: 0.0033s\n",
      "Epoch: 050 loss_train: 1.6074 acc_train: 0.3682 loss_val: 1.5905 acc_val: 0.3413 time: 0.0031s\n",
      "Epoch: 051 loss_train: 1.5900 acc_train: 0.3738 loss_val: 1.5812 acc_val: 0.3506 time: 0.0031s\n",
      "Epoch: 052 loss_train: 1.5893 acc_train: 0.3849 loss_val: 1.5717 acc_val: 0.3561 time: 0.0034s\n",
      "Epoch: 053 loss_train: 1.5749 acc_train: 0.3922 loss_val: 1.5621 acc_val: 0.3690 time: 0.0034s\n",
      "Epoch: 054 loss_train: 1.5668 acc_train: 0.3984 loss_val: 1.5525 acc_val: 0.3782 time: 0.0034s\n",
      "Epoch: 055 loss_train: 1.5509 acc_train: 0.4150 loss_val: 1.5428 acc_val: 0.3893 time: 0.0031s\n",
      "Epoch: 056 loss_train: 1.5472 acc_train: 0.4181 loss_val: 1.5330 acc_val: 0.3967 time: 0.0031s\n",
      "Epoch: 057 loss_train: 1.5350 acc_train: 0.4304 loss_val: 1.5232 acc_val: 0.4022 time: 0.0032s\n",
      "Epoch: 058 loss_train: 1.5283 acc_train: 0.4286 loss_val: 1.5133 acc_val: 0.4114 time: 0.0031s\n",
      "Epoch: 059 loss_train: 1.5150 acc_train: 0.4440 loss_val: 1.5035 acc_val: 0.4244 time: 0.0031s\n",
      "Epoch: 060 loss_train: 1.4990 acc_train: 0.4483 loss_val: 1.4935 acc_val: 0.4336 time: 0.0031s\n",
      "Epoch: 061 loss_train: 1.4960 acc_train: 0.4440 loss_val: 1.4836 acc_val: 0.4354 time: 0.0034s\n",
      "Epoch: 062 loss_train: 1.4879 acc_train: 0.4575 loss_val: 1.4737 acc_val: 0.4428 time: 0.0034s\n",
      "Epoch: 063 loss_train: 1.4742 acc_train: 0.4563 loss_val: 1.4638 acc_val: 0.4483 time: 0.0031s\n",
      "Epoch: 064 loss_train: 1.4673 acc_train: 0.4643 loss_val: 1.4538 acc_val: 0.4594 time: 0.0031s\n",
      "Epoch: 065 loss_train: 1.4636 acc_train: 0.4538 loss_val: 1.4439 acc_val: 0.4631 time: 0.0032s\n",
      "Epoch: 066 loss_train: 1.4491 acc_train: 0.4711 loss_val: 1.4340 acc_val: 0.4705 time: 0.0031s\n",
      "Epoch: 067 loss_train: 1.4386 acc_train: 0.4692 loss_val: 1.4240 acc_val: 0.4797 time: 0.0032s\n",
      "Epoch: 068 loss_train: 1.4304 acc_train: 0.4852 loss_val: 1.4141 acc_val: 0.5037 time: 0.0033s\n",
      "Epoch: 069 loss_train: 1.4249 acc_train: 0.4865 loss_val: 1.4044 acc_val: 0.5185 time: 0.0031s\n",
      "Epoch: 070 loss_train: 1.4170 acc_train: 0.5049 loss_val: 1.3947 acc_val: 0.5295 time: 0.0031s\n",
      "Epoch: 071 loss_train: 1.4039 acc_train: 0.5092 loss_val: 1.3852 acc_val: 0.5461 time: 0.0034s\n",
      "Epoch: 072 loss_train: 1.4014 acc_train: 0.5259 loss_val: 1.3759 acc_val: 0.5590 time: 0.0035s\n",
      "Epoch: 073 loss_train: 1.3829 acc_train: 0.5320 loss_val: 1.3666 acc_val: 0.5664 time: 0.0031s\n",
      "Epoch: 074 loss_train: 1.3717 acc_train: 0.5376 loss_val: 1.3573 acc_val: 0.5683 time: 0.0031s\n",
      "Epoch: 075 loss_train: 1.3711 acc_train: 0.5333 loss_val: 1.3483 acc_val: 0.5738 time: 0.0031s\n",
      "Epoch: 076 loss_train: 1.3676 acc_train: 0.5376 loss_val: 1.3394 acc_val: 0.5756 time: 0.0031s\n",
      "Epoch: 077 loss_train: 1.3480 acc_train: 0.5493 loss_val: 1.3305 acc_val: 0.5756 time: 0.0031s\n",
      "Epoch: 078 loss_train: 1.3456 acc_train: 0.5351 loss_val: 1.3217 acc_val: 0.5775 time: 0.0032s\n",
      "Epoch: 079 loss_train: 1.3367 acc_train: 0.5573 loss_val: 1.3129 acc_val: 0.5830 time: 0.0034s\n",
      "Epoch: 080 loss_train: 1.3250 acc_train: 0.5400 loss_val: 1.3041 acc_val: 0.5830 time: 0.0035s\n",
      "Epoch: 081 loss_train: 1.3239 acc_train: 0.5511 loss_val: 1.2955 acc_val: 0.5886 time: 0.0034s\n",
      "Epoch: 082 loss_train: 1.3161 acc_train: 0.5677 loss_val: 1.2870 acc_val: 0.5978 time: 0.0031s\n",
      "Epoch: 083 loss_train: 1.2969 acc_train: 0.5813 loss_val: 1.2786 acc_val: 0.6070 time: 0.0031s\n",
      "Epoch: 084 loss_train: 1.2915 acc_train: 0.5702 loss_val: 1.2703 acc_val: 0.6125 time: 0.0031s\n",
      "Epoch: 085 loss_train: 1.2798 acc_train: 0.5893 loss_val: 1.2621 acc_val: 0.6162 time: 0.0031s\n",
      "Epoch: 086 loss_train: 1.2787 acc_train: 0.5825 loss_val: 1.2541 acc_val: 0.6199 time: 0.0031s\n",
      "Epoch: 087 loss_train: 1.2693 acc_train: 0.5979 loss_val: 1.2460 acc_val: 0.6236 time: 0.0031s\n",
      "Epoch: 088 loss_train: 1.2645 acc_train: 0.5930 loss_val: 1.2380 acc_val: 0.6310 time: 0.0038s\n",
      "Epoch: 089 loss_train: 1.2509 acc_train: 0.5881 loss_val: 1.2300 acc_val: 0.6328 time: 0.0035s\n",
      "Epoch: 090 loss_train: 1.2414 acc_train: 0.6065 loss_val: 1.2222 acc_val: 0.6347 time: 0.0032s\n",
      "Epoch: 091 loss_train: 1.2313 acc_train: 0.6158 loss_val: 1.2144 acc_val: 0.6439 time: 0.0031s\n",
      "Epoch: 092 loss_train: 1.2332 acc_train: 0.6158 loss_val: 1.2068 acc_val: 0.6513 time: 0.0033s\n",
      "Epoch: 093 loss_train: 1.2341 acc_train: 0.6176 loss_val: 1.1993 acc_val: 0.6568 time: 0.0032s\n",
      "Epoch: 094 loss_train: 1.2062 acc_train: 0.6392 loss_val: 1.1918 acc_val: 0.6605 time: 0.0033s\n",
      "Epoch: 095 loss_train: 1.2201 acc_train: 0.6256 loss_val: 1.1844 acc_val: 0.6661 time: 0.0034s\n",
      "Epoch: 096 loss_train: 1.2042 acc_train: 0.6373 loss_val: 1.1770 acc_val: 0.6679 time: 0.0036s\n",
      "Epoch: 097 loss_train: 1.1914 acc_train: 0.6533 loss_val: 1.1698 acc_val: 0.6734 time: 0.0037s\n",
      "Epoch: 098 loss_train: 1.1865 acc_train: 0.6453 loss_val: 1.1627 acc_val: 0.6753 time: 0.0032s\n",
      "Epoch: 099 loss_train: 1.1785 acc_train: 0.6361 loss_val: 1.1557 acc_val: 0.6753 time: 0.0031s\n",
      "Epoch: 100 loss_train: 1.1758 acc_train: 0.6373 loss_val: 1.1489 acc_val: 0.6753 time: 0.0031s\n",
      "Epoch: 101 loss_train: 1.1773 acc_train: 0.6478 loss_val: 1.1421 acc_val: 0.6827 time: 0.0031s\n",
      "Epoch: 102 loss_train: 1.1674 acc_train: 0.6336 loss_val: 1.1354 acc_val: 0.6863 time: 0.0032s\n",
      "Epoch: 103 loss_train: 1.1524 acc_train: 0.6466 loss_val: 1.1288 acc_val: 0.6900 time: 0.0034s\n",
      "Epoch: 104 loss_train: 1.1481 acc_train: 0.6589 loss_val: 1.1224 acc_val: 0.6937 time: 0.0032s\n",
      "Epoch: 105 loss_train: 1.1367 acc_train: 0.6675 loss_val: 1.1159 acc_val: 0.6956 time: 0.0034s\n",
      "Epoch: 106 loss_train: 1.1374 acc_train: 0.6656 loss_val: 1.1092 acc_val: 0.6993 time: 0.0034s\n",
      "Epoch: 107 loss_train: 1.1340 acc_train: 0.6595 loss_val: 1.1026 acc_val: 0.7030 time: 0.0032s\n",
      "Epoch: 108 loss_train: 1.1314 acc_train: 0.6564 loss_val: 1.0961 acc_val: 0.7103 time: 0.0031s\n",
      "Epoch: 109 loss_train: 1.1166 acc_train: 0.6749 loss_val: 1.0897 acc_val: 0.7140 time: 0.0031s\n",
      "Epoch: 110 loss_train: 1.1218 acc_train: 0.6780 loss_val: 1.0833 acc_val: 0.7177 time: 0.0032s\n",
      "Epoch: 111 loss_train: 1.0982 acc_train: 0.6823 loss_val: 1.0770 acc_val: 0.7122 time: 0.0031s\n",
      "Epoch: 112 loss_train: 1.0918 acc_train: 0.6792 loss_val: 1.0709 acc_val: 0.7140 time: 0.0032s\n",
      "Epoch: 113 loss_train: 1.0926 acc_train: 0.6860 loss_val: 1.0650 acc_val: 0.7140 time: 0.0034s\n",
      "Epoch: 114 loss_train: 1.0795 acc_train: 0.6946 loss_val: 1.0591 acc_val: 0.7196 time: 0.0034s\n",
      "Epoch: 115 loss_train: 1.0791 acc_train: 0.6970 loss_val: 1.0533 acc_val: 0.7196 time: 0.0034s\n",
      "Epoch: 116 loss_train: 1.0783 acc_train: 0.6884 loss_val: 1.0476 acc_val: 0.7214 time: 0.0031s\n",
      "Epoch: 117 loss_train: 1.0744 acc_train: 0.6817 loss_val: 1.0419 acc_val: 0.7251 time: 0.0031s\n",
      "Epoch: 118 loss_train: 1.0748 acc_train: 0.6995 loss_val: 1.0363 acc_val: 0.7269 time: 0.0033s\n",
      "Epoch: 119 loss_train: 1.0565 acc_train: 0.7026 loss_val: 1.0308 acc_val: 0.7306 time: 0.0031s\n",
      "Epoch: 120 loss_train: 1.0607 acc_train: 0.6940 loss_val: 1.0254 acc_val: 0.7306 time: 0.0031s\n",
      "Epoch: 121 loss_train: 1.0540 acc_train: 0.7032 loss_val: 1.0200 acc_val: 0.7343 time: 0.0031s\n",
      "Epoch: 122 loss_train: 1.0413 acc_train: 0.7106 loss_val: 1.0145 acc_val: 0.7380 time: 0.0032s\n",
      "Epoch: 123 loss_train: 1.0470 acc_train: 0.7186 loss_val: 1.0088 acc_val: 0.7399 time: 0.0033s\n",
      "Epoch: 124 loss_train: 1.0335 acc_train: 0.7149 loss_val: 1.0030 acc_val: 0.7380 time: 0.0033s\n",
      "Epoch: 125 loss_train: 1.0347 acc_train: 0.7186 loss_val: 0.9974 acc_val: 0.7454 time: 0.0031s\n",
      "Epoch: 126 loss_train: 1.0240 acc_train: 0.7229 loss_val: 0.9918 acc_val: 0.7491 time: 0.0031s\n",
      "Epoch: 127 loss_train: 1.0089 acc_train: 0.7229 loss_val: 0.9862 acc_val: 0.7491 time: 0.0031s\n",
      "Epoch: 128 loss_train: 1.0166 acc_train: 0.7328 loss_val: 0.9808 acc_val: 0.7528 time: 0.0032s\n",
      "Epoch: 129 loss_train: 1.0106 acc_train: 0.7469 loss_val: 0.9754 acc_val: 0.7583 time: 0.0031s\n",
      "Epoch: 130 loss_train: 1.0115 acc_train: 0.7241 loss_val: 0.9703 acc_val: 0.7620 time: 0.0031s\n",
      "Epoch: 131 loss_train: 0.9948 acc_train: 0.7352 loss_val: 0.9653 acc_val: 0.7638 time: 0.0031s\n",
      "Epoch: 132 loss_train: 0.9949 acc_train: 0.7321 loss_val: 0.9602 acc_val: 0.7749 time: 0.0034s\n",
      "Epoch: 133 loss_train: 0.9829 acc_train: 0.7432 loss_val: 0.9550 acc_val: 0.7749 time: 0.0034s\n",
      "Epoch: 134 loss_train: 0.9827 acc_train: 0.7426 loss_val: 0.9497 acc_val: 0.7804 time: 0.0033s\n",
      "Epoch: 135 loss_train: 0.9883 acc_train: 0.7420 loss_val: 0.9447 acc_val: 0.7841 time: 0.0031s\n",
      "Epoch: 136 loss_train: 0.9717 acc_train: 0.7469 loss_val: 0.9398 acc_val: 0.7878 time: 0.0031s\n",
      "Epoch: 137 loss_train: 0.9916 acc_train: 0.7469 loss_val: 0.9351 acc_val: 0.7897 time: 0.0032s\n",
      "Epoch: 138 loss_train: 0.9678 acc_train: 0.7451 loss_val: 0.9306 acc_val: 0.7897 time: 0.0031s\n",
      "Epoch: 139 loss_train: 0.9652 acc_train: 0.7383 loss_val: 0.9263 acc_val: 0.7934 time: 0.0031s\n",
      "Epoch: 140 loss_train: 0.9630 acc_train: 0.7414 loss_val: 0.9222 acc_val: 0.7915 time: 0.0031s\n",
      "Epoch: 141 loss_train: 0.9504 acc_train: 0.7543 loss_val: 0.9179 acc_val: 0.7934 time: 0.0032s\n",
      "Epoch: 142 loss_train: 0.9531 acc_train: 0.7635 loss_val: 0.9137 acc_val: 0.7934 time: 0.0031s\n",
      "Epoch: 143 loss_train: 0.9403 acc_train: 0.7703 loss_val: 0.9095 acc_val: 0.7970 time: 0.0034s\n",
      "Epoch: 144 loss_train: 0.9534 acc_train: 0.7635 loss_val: 0.9053 acc_val: 0.8007 time: 0.0033s\n",
      "Epoch: 145 loss_train: 0.9540 acc_train: 0.7562 loss_val: 0.9014 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 146 loss_train: 0.9424 acc_train: 0.7654 loss_val: 0.8977 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 147 loss_train: 0.9282 acc_train: 0.7703 loss_val: 0.8937 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 148 loss_train: 0.9194 acc_train: 0.7796 loss_val: 0.8896 acc_val: 0.8100 time: 0.0031s\n",
      "Epoch: 149 loss_train: 0.9258 acc_train: 0.7697 loss_val: 0.8854 acc_val: 0.8100 time: 0.0031s\n",
      "Epoch: 150 loss_train: 0.9127 acc_train: 0.7826 loss_val: 0.8813 acc_val: 0.8100 time: 0.0039s\n",
      "Epoch: 151 loss_train: 0.9219 acc_train: 0.7734 loss_val: 0.8773 acc_val: 0.8118 time: 0.0034s\n",
      "Epoch: 152 loss_train: 0.9178 acc_train: 0.7654 loss_val: 0.8735 acc_val: 0.8118 time: 0.0031s\n",
      "Epoch: 153 loss_train: 0.9269 acc_train: 0.7635 loss_val: 0.8698 acc_val: 0.8155 time: 0.0032s\n",
      "Epoch: 154 loss_train: 0.9144 acc_train: 0.7716 loss_val: 0.8663 acc_val: 0.8155 time: 0.0033s\n",
      "Epoch: 155 loss_train: 0.9161 acc_train: 0.7789 loss_val: 0.8631 acc_val: 0.8137 time: 0.0033s\n",
      "Epoch: 156 loss_train: 0.9028 acc_train: 0.7796 loss_val: 0.8600 acc_val: 0.8137 time: 0.0033s\n",
      "Epoch: 157 loss_train: 0.8907 acc_train: 0.7789 loss_val: 0.8569 acc_val: 0.8118 time: 0.0033s\n",
      "Epoch: 158 loss_train: 0.8982 acc_train: 0.7783 loss_val: 0.8537 acc_val: 0.8118 time: 0.0034s\n",
      "Epoch: 159 loss_train: 0.8984 acc_train: 0.7716 loss_val: 0.8508 acc_val: 0.8137 time: 0.0034s\n",
      "Epoch: 160 loss_train: 0.8905 acc_train: 0.7833 loss_val: 0.8478 acc_val: 0.8192 time: 0.0031s\n",
      "Epoch: 161 loss_train: 0.8825 acc_train: 0.7839 loss_val: 0.8447 acc_val: 0.8192 time: 0.0032s\n",
      "Epoch: 162 loss_train: 0.9004 acc_train: 0.7765 loss_val: 0.8418 acc_val: 0.8173 time: 0.0031s\n",
      "Epoch: 163 loss_train: 0.8726 acc_train: 0.7851 loss_val: 0.8386 acc_val: 0.8210 time: 0.0031s\n",
      "Epoch: 164 loss_train: 0.8712 acc_train: 0.7974 loss_val: 0.8354 acc_val: 0.8229 time: 0.0031s\n",
      "Epoch: 165 loss_train: 0.8792 acc_train: 0.7876 loss_val: 0.8322 acc_val: 0.8247 time: 0.0031s\n",
      "Epoch: 166 loss_train: 0.8727 acc_train: 0.7906 loss_val: 0.8290 acc_val: 0.8266 time: 0.0034s\n",
      "Epoch: 167 loss_train: 0.8683 acc_train: 0.7900 loss_val: 0.8260 acc_val: 0.8266 time: 0.0034s\n",
      "Epoch: 168 loss_train: 0.8667 acc_train: 0.8103 loss_val: 0.8232 acc_val: 0.8266 time: 0.0034s\n",
      "Epoch: 169 loss_train: 0.8712 acc_train: 0.7783 loss_val: 0.8206 acc_val: 0.8266 time: 0.0034s\n",
      "Epoch: 170 loss_train: 0.8652 acc_train: 0.7777 loss_val: 0.8181 acc_val: 0.8266 time: 0.0031s\n",
      "Epoch: 171 loss_train: 0.8632 acc_train: 0.7876 loss_val: 0.8156 acc_val: 0.8284 time: 0.0032s\n",
      "Epoch: 172 loss_train: 0.8500 acc_train: 0.7906 loss_val: 0.8132 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 173 loss_train: 0.8481 acc_train: 0.8085 loss_val: 0.8105 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 174 loss_train: 0.8487 acc_train: 0.7894 loss_val: 0.8078 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 175 loss_train: 0.8500 acc_train: 0.8116 loss_val: 0.8051 acc_val: 0.8358 time: 0.0032s\n",
      "Epoch: 176 loss_train: 0.8467 acc_train: 0.7974 loss_val: 0.8025 acc_val: 0.8358 time: 0.0034s\n",
      "Epoch: 177 loss_train: 0.8359 acc_train: 0.8073 loss_val: 0.7999 acc_val: 0.8358 time: 0.0034s\n",
      "Epoch: 178 loss_train: 0.8316 acc_train: 0.8103 loss_val: 0.7970 acc_val: 0.8358 time: 0.0033s\n",
      "Epoch: 179 loss_train: 0.8425 acc_train: 0.8060 loss_val: 0.7944 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 180 loss_train: 0.8338 acc_train: 0.8116 loss_val: 0.7918 acc_val: 0.8376 time: 0.0031s\n",
      "Epoch: 181 loss_train: 0.8215 acc_train: 0.8097 loss_val: 0.7892 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 182 loss_train: 0.8149 acc_train: 0.8165 loss_val: 0.7863 acc_val: 0.8321 time: 0.0033s\n",
      "Epoch: 183 loss_train: 0.8265 acc_train: 0.8042 loss_val: 0.7836 acc_val: 0.8358 time: 0.0033s\n",
      "Epoch: 184 loss_train: 0.8213 acc_train: 0.8171 loss_val: 0.7810 acc_val: 0.8358 time: 0.0032s\n",
      "Epoch: 185 loss_train: 0.8156 acc_train: 0.8097 loss_val: 0.7786 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 186 loss_train: 0.8238 acc_train: 0.8134 loss_val: 0.7766 acc_val: 0.8358 time: 0.0032s\n",
      "Epoch: 187 loss_train: 0.8257 acc_train: 0.7999 loss_val: 0.7749 acc_val: 0.8358 time: 0.0035s\n",
      "Epoch: 188 loss_train: 0.8183 acc_train: 0.8054 loss_val: 0.7733 acc_val: 0.8358 time: 0.0034s\n",
      "Epoch: 189 loss_train: 0.8109 acc_train: 0.8079 loss_val: 0.7717 acc_val: 0.8358 time: 0.0032s\n",
      "Epoch: 190 loss_train: 0.8098 acc_train: 0.8116 loss_val: 0.7701 acc_val: 0.8321 time: 0.0032s\n",
      "Epoch: 191 loss_train: 0.8123 acc_train: 0.8122 loss_val: 0.7682 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 192 loss_train: 0.8097 acc_train: 0.8128 loss_val: 0.7661 acc_val: 0.8321 time: 0.0032s\n",
      "Epoch: 193 loss_train: 0.7964 acc_train: 0.8171 loss_val: 0.7638 acc_val: 0.8339 time: 0.0032s\n",
      "Epoch: 194 loss_train: 0.7890 acc_train: 0.8264 loss_val: 0.7612 acc_val: 0.8339 time: 0.0034s\n",
      "Epoch: 195 loss_train: 0.7855 acc_train: 0.8307 loss_val: 0.7584 acc_val: 0.8376 time: 0.0034s\n",
      "Epoch: 196 loss_train: 0.8016 acc_train: 0.8227 loss_val: 0.7556 acc_val: 0.8358 time: 0.0031s\n",
      "Epoch: 197 loss_train: 0.7815 acc_train: 0.8171 loss_val: 0.7529 acc_val: 0.8376 time: 0.0031s\n",
      "Epoch: 198 loss_train: 0.7769 acc_train: 0.8171 loss_val: 0.7502 acc_val: 0.8376 time: 0.0032s\n",
      "Epoch: 199 loss_train: 0.7734 acc_train: 0.8276 loss_val: 0.7480 acc_val: 0.8395 time: 0.0033s\n",
      "Epoch: 200 loss_train: 0.7927 acc_train: 0.8147 loss_val: 0.7461 acc_val: 0.8376 time: 0.0034s\n",
      "Test set results: loss= 0.7764 accuracy= 0.8303\n",
      "Training the model for the 4th time\n",
      "Epoch: 001 loss_train: 1.9454 acc_train: 0.1521 loss_val: 1.9407 acc_val: 0.1531 time: 0.0034s\n",
      "Epoch: 002 loss_train: 1.9412 acc_train: 0.1817 loss_val: 1.9373 acc_val: 0.1476 time: 0.0032s\n",
      "Epoch: 003 loss_train: 1.9379 acc_train: 0.1810 loss_val: 1.9338 acc_val: 0.1476 time: 0.0032s\n",
      "Epoch: 004 loss_train: 1.9343 acc_train: 0.1730 loss_val: 1.9300 acc_val: 0.1476 time: 0.0031s\n",
      "Epoch: 005 loss_train: 1.9300 acc_train: 0.1700 loss_val: 1.9260 acc_val: 0.1476 time: 0.0031s\n",
      "Epoch: 006 loss_train: 1.9262 acc_train: 0.1693 loss_val: 1.9217 acc_val: 0.1458 time: 0.0032s\n",
      "Epoch: 007 loss_train: 1.9217 acc_train: 0.1736 loss_val: 1.9171 acc_val: 0.1476 time: 0.0032s\n",
      "Epoch: 008 loss_train: 1.9170 acc_train: 0.1736 loss_val: 1.9122 acc_val: 0.1476 time: 0.0032s\n",
      "Epoch: 009 loss_train: 1.9118 acc_train: 0.1730 loss_val: 1.9070 acc_val: 0.1476 time: 0.0034s\n",
      "Epoch: 010 loss_train: 1.9060 acc_train: 0.1724 loss_val: 1.9016 acc_val: 0.1458 time: 0.0034s\n",
      "Epoch: 011 loss_train: 1.9014 acc_train: 0.1872 loss_val: 1.8960 acc_val: 0.1531 time: 0.0038s\n",
      "Epoch: 012 loss_train: 1.8957 acc_train: 0.2094 loss_val: 1.8902 acc_val: 0.1568 time: 0.0035s\n",
      "Epoch: 013 loss_train: 1.8894 acc_train: 0.2278 loss_val: 1.8841 acc_val: 0.1679 time: 0.0032s\n",
      "Epoch: 014 loss_train: 1.8833 acc_train: 0.2426 loss_val: 1.8779 acc_val: 0.2251 time: 0.0032s\n",
      "Epoch: 015 loss_train: 1.8758 acc_train: 0.2623 loss_val: 1.8714 acc_val: 0.2897 time: 0.0034s\n",
      "Epoch: 016 loss_train: 1.8710 acc_train: 0.2876 loss_val: 1.8648 acc_val: 0.3450 time: 0.0034s\n",
      "Epoch: 017 loss_train: 1.8642 acc_train: 0.2956 loss_val: 1.8580 acc_val: 0.3764 time: 0.0032s\n",
      "Epoch: 018 loss_train: 1.8568 acc_train: 0.3017 loss_val: 1.8512 acc_val: 0.3672 time: 0.0032s\n",
      "Epoch: 019 loss_train: 1.8489 acc_train: 0.3190 loss_val: 1.8442 acc_val: 0.3413 time: 0.0032s\n",
      "Epoch: 020 loss_train: 1.8428 acc_train: 0.3110 loss_val: 1.8372 acc_val: 0.3266 time: 0.0033s\n",
      "Epoch: 021 loss_train: 1.8371 acc_train: 0.3270 loss_val: 1.8302 acc_val: 0.3192 time: 0.0035s\n",
      "Epoch: 022 loss_train: 1.8271 acc_train: 0.3134 loss_val: 1.8233 acc_val: 0.3173 time: 0.0034s\n",
      "Epoch: 023 loss_train: 1.8252 acc_train: 0.3202 loss_val: 1.8164 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 024 loss_train: 1.8180 acc_train: 0.3067 loss_val: 1.8097 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 025 loss_train: 1.8079 acc_train: 0.3085 loss_val: 1.8032 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 026 loss_train: 1.8009 acc_train: 0.3085 loss_val: 1.7968 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 027 loss_train: 1.7959 acc_train: 0.3091 loss_val: 1.7906 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 028 loss_train: 1.7899 acc_train: 0.3048 loss_val: 1.7847 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 029 loss_train: 1.7817 acc_train: 0.3011 loss_val: 1.7789 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 030 loss_train: 1.7788 acc_train: 0.3023 loss_val: 1.7732 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 031 loss_train: 1.7758 acc_train: 0.3011 loss_val: 1.7678 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 032 loss_train: 1.7737 acc_train: 0.3011 loss_val: 1.7624 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 033 loss_train: 1.7650 acc_train: 0.3023 loss_val: 1.7571 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 034 loss_train: 1.7649 acc_train: 0.2986 loss_val: 1.7518 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 035 loss_train: 1.7568 acc_train: 0.3017 loss_val: 1.7463 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 036 loss_train: 1.7518 acc_train: 0.3030 loss_val: 1.7407 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 037 loss_train: 1.7410 acc_train: 0.3017 loss_val: 1.7349 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 038 loss_train: 1.7353 acc_train: 0.2986 loss_val: 1.7290 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 039 loss_train: 1.7336 acc_train: 0.2993 loss_val: 1.7229 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 040 loss_train: 1.7265 acc_train: 0.3048 loss_val: 1.7166 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 041 loss_train: 1.7252 acc_train: 0.3023 loss_val: 1.7101 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 042 loss_train: 1.7205 acc_train: 0.3054 loss_val: 1.7034 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 043 loss_train: 1.7078 acc_train: 0.3023 loss_val: 1.6967 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 044 loss_train: 1.7018 acc_train: 0.3091 loss_val: 1.6899 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 045 loss_train: 1.6961 acc_train: 0.3060 loss_val: 1.6830 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 046 loss_train: 1.6948 acc_train: 0.3134 loss_val: 1.6761 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 047 loss_train: 1.6850 acc_train: 0.3159 loss_val: 1.6691 acc_val: 0.3155 time: 0.0032s\n",
      "Epoch: 048 loss_train: 1.6863 acc_train: 0.3196 loss_val: 1.6620 acc_val: 0.3155 time: 0.0031s\n",
      "Epoch: 049 loss_train: 1.6668 acc_train: 0.3171 loss_val: 1.6549 acc_val: 0.3155 time: 0.0033s\n",
      "Epoch: 050 loss_train: 1.6670 acc_train: 0.3208 loss_val: 1.6476 acc_val: 0.3192 time: 0.0035s\n",
      "Epoch: 051 loss_train: 1.6549 acc_train: 0.3337 loss_val: 1.6401 acc_val: 0.3247 time: 0.0034s\n",
      "Epoch: 052 loss_train: 1.6514 acc_train: 0.3319 loss_val: 1.6325 acc_val: 0.3247 time: 0.0034s\n",
      "Epoch: 053 loss_train: 1.6357 acc_train: 0.3424 loss_val: 1.6247 acc_val: 0.3303 time: 0.0032s\n",
      "Epoch: 054 loss_train: 1.6320 acc_train: 0.3411 loss_val: 1.6168 acc_val: 0.3321 time: 0.0031s\n",
      "Epoch: 055 loss_train: 1.6180 acc_train: 0.3498 loss_val: 1.6087 acc_val: 0.3339 time: 0.0033s\n",
      "Epoch: 056 loss_train: 1.6114 acc_train: 0.3621 loss_val: 1.6005 acc_val: 0.3376 time: 0.0031s\n",
      "Epoch: 057 loss_train: 1.6072 acc_train: 0.3744 loss_val: 1.5921 acc_val: 0.3413 time: 0.0032s\n",
      "Epoch: 058 loss_train: 1.6049 acc_train: 0.3695 loss_val: 1.5836 acc_val: 0.3524 time: 0.0032s\n",
      "Epoch: 059 loss_train: 1.5912 acc_train: 0.3879 loss_val: 1.5749 acc_val: 0.3598 time: 0.0034s\n",
      "Epoch: 060 loss_train: 1.5793 acc_train: 0.3879 loss_val: 1.5661 acc_val: 0.3690 time: 0.0034s\n",
      "Epoch: 061 loss_train: 1.5829 acc_train: 0.4076 loss_val: 1.5571 acc_val: 0.3801 time: 0.0033s\n",
      "Epoch: 062 loss_train: 1.5630 acc_train: 0.4126 loss_val: 1.5479 acc_val: 0.3893 time: 0.0033s\n",
      "Epoch: 063 loss_train: 1.5570 acc_train: 0.4261 loss_val: 1.5386 acc_val: 0.4077 time: 0.0031s\n",
      "Epoch: 064 loss_train: 1.5500 acc_train: 0.4280 loss_val: 1.5292 acc_val: 0.4188 time: 0.0031s\n",
      "Epoch: 065 loss_train: 1.5376 acc_train: 0.4378 loss_val: 1.5196 acc_val: 0.4299 time: 0.0031s\n",
      "Epoch: 066 loss_train: 1.5256 acc_train: 0.4507 loss_val: 1.5099 acc_val: 0.4410 time: 0.0032s\n",
      "Epoch: 067 loss_train: 1.5218 acc_train: 0.4563 loss_val: 1.5003 acc_val: 0.4446 time: 0.0031s\n",
      "Epoch: 068 loss_train: 1.5125 acc_train: 0.4741 loss_val: 1.4906 acc_val: 0.4539 time: 0.0031s\n",
      "Epoch: 069 loss_train: 1.4988 acc_train: 0.4791 loss_val: 1.4809 acc_val: 0.4686 time: 0.0031s\n",
      "Epoch: 070 loss_train: 1.4880 acc_train: 0.4858 loss_val: 1.4711 acc_val: 0.4908 time: 0.0034s\n",
      "Epoch: 071 loss_train: 1.4847 acc_train: 0.4920 loss_val: 1.4613 acc_val: 0.5000 time: 0.0034s\n",
      "Epoch: 072 loss_train: 1.4724 acc_train: 0.4920 loss_val: 1.4514 acc_val: 0.5092 time: 0.0038s\n",
      "Epoch: 073 loss_train: 1.4648 acc_train: 0.5031 loss_val: 1.4415 acc_val: 0.5148 time: 0.0033s\n",
      "Epoch: 074 loss_train: 1.4574 acc_train: 0.5105 loss_val: 1.4317 acc_val: 0.5277 time: 0.0034s\n",
      "Epoch: 075 loss_train: 1.4402 acc_train: 0.5289 loss_val: 1.4219 acc_val: 0.5332 time: 0.0032s\n",
      "Epoch: 076 loss_train: 1.4367 acc_train: 0.5234 loss_val: 1.4122 acc_val: 0.5406 time: 0.0033s\n",
      "Epoch: 077 loss_train: 1.4329 acc_train: 0.5314 loss_val: 1.4027 acc_val: 0.5461 time: 0.0032s\n",
      "Epoch: 078 loss_train: 1.4131 acc_train: 0.5400 loss_val: 1.3933 acc_val: 0.5517 time: 0.0032s\n",
      "Epoch: 079 loss_train: 1.4108 acc_train: 0.5443 loss_val: 1.3840 acc_val: 0.5646 time: 0.0033s\n",
      "Epoch: 080 loss_train: 1.4009 acc_train: 0.5647 loss_val: 1.3748 acc_val: 0.5701 time: 0.0033s\n",
      "Epoch: 081 loss_train: 1.4034 acc_train: 0.5431 loss_val: 1.3659 acc_val: 0.5775 time: 0.0035s\n",
      "Epoch: 082 loss_train: 1.3882 acc_train: 0.5579 loss_val: 1.3569 acc_val: 0.5867 time: 0.0034s\n",
      "Epoch: 083 loss_train: 1.3809 acc_train: 0.5677 loss_val: 1.3481 acc_val: 0.5904 time: 0.0035s\n",
      "Epoch: 084 loss_train: 1.3616 acc_train: 0.5696 loss_val: 1.3392 acc_val: 0.5959 time: 0.0031s\n",
      "Epoch: 085 loss_train: 1.3658 acc_train: 0.5733 loss_val: 1.3304 acc_val: 0.6052 time: 0.0033s\n",
      "Epoch: 086 loss_train: 1.3593 acc_train: 0.5868 loss_val: 1.3218 acc_val: 0.6107 time: 0.0033s\n",
      "Epoch: 087 loss_train: 1.3387 acc_train: 0.5887 loss_val: 1.3133 acc_val: 0.6199 time: 0.0031s\n",
      "Epoch: 088 loss_train: 1.3410 acc_train: 0.5837 loss_val: 1.3049 acc_val: 0.6236 time: 0.0031s\n",
      "Epoch: 089 loss_train: 1.3262 acc_train: 0.5954 loss_val: 1.2964 acc_val: 0.6273 time: 0.0031s\n",
      "Epoch: 090 loss_train: 1.3122 acc_train: 0.6004 loss_val: 1.2879 acc_val: 0.6292 time: 0.0032s\n",
      "Epoch: 091 loss_train: 1.3152 acc_train: 0.5739 loss_val: 1.2796 acc_val: 0.6347 time: 0.0033s\n",
      "Epoch: 092 loss_train: 1.2957 acc_train: 0.6016 loss_val: 1.2712 acc_val: 0.6402 time: 0.0035s\n",
      "Epoch: 093 loss_train: 1.2852 acc_train: 0.6299 loss_val: 1.2630 acc_val: 0.6513 time: 0.0034s\n",
      "Epoch: 094 loss_train: 1.2909 acc_train: 0.6145 loss_val: 1.2549 acc_val: 0.6531 time: 0.0031s\n",
      "Epoch: 095 loss_train: 1.2777 acc_train: 0.6213 loss_val: 1.2469 acc_val: 0.6642 time: 0.0031s\n",
      "Epoch: 096 loss_train: 1.2666 acc_train: 0.6299 loss_val: 1.2390 acc_val: 0.6661 time: 0.0031s\n",
      "Epoch: 097 loss_train: 1.2719 acc_train: 0.6145 loss_val: 1.2315 acc_val: 0.6661 time: 0.0031s\n",
      "Epoch: 098 loss_train: 1.2587 acc_train: 0.6262 loss_val: 1.2242 acc_val: 0.6661 time: 0.0034s\n",
      "Epoch: 099 loss_train: 1.2424 acc_train: 0.6416 loss_val: 1.2171 acc_val: 0.6661 time: 0.0031s\n",
      "Epoch: 100 loss_train: 1.2418 acc_train: 0.6293 loss_val: 1.2100 acc_val: 0.6679 time: 0.0031s\n",
      "Epoch: 101 loss_train: 1.2230 acc_train: 0.6515 loss_val: 1.2031 acc_val: 0.6716 time: 0.0034s\n",
      "Epoch: 102 loss_train: 1.2203 acc_train: 0.6447 loss_val: 1.1960 acc_val: 0.6753 time: 0.0034s\n",
      "Epoch: 103 loss_train: 1.2337 acc_train: 0.6459 loss_val: 1.1890 acc_val: 0.6771 time: 0.0031s\n",
      "Epoch: 104 loss_train: 1.2199 acc_train: 0.6367 loss_val: 1.1821 acc_val: 0.6827 time: 0.0031s\n",
      "Epoch: 105 loss_train: 1.2095 acc_train: 0.6459 loss_val: 1.1752 acc_val: 0.6863 time: 0.0031s\n",
      "Epoch: 106 loss_train: 1.1910 acc_train: 0.6447 loss_val: 1.1682 acc_val: 0.6919 time: 0.0031s\n",
      "Epoch: 107 loss_train: 1.1986 acc_train: 0.6502 loss_val: 1.1614 acc_val: 0.6993 time: 0.0032s\n",
      "Epoch: 108 loss_train: 1.1900 acc_train: 0.6663 loss_val: 1.1547 acc_val: 0.7011 time: 0.0032s\n",
      "Epoch: 109 loss_train: 1.1790 acc_train: 0.6595 loss_val: 1.1480 acc_val: 0.7048 time: 0.0032s\n",
      "Epoch: 110 loss_train: 1.1859 acc_train: 0.6570 loss_val: 1.1417 acc_val: 0.7048 time: 0.0031s\n",
      "Epoch: 111 loss_train: 1.1746 acc_train: 0.6570 loss_val: 1.1356 acc_val: 0.7030 time: 0.0034s\n",
      "Epoch: 112 loss_train: 1.1666 acc_train: 0.6490 loss_val: 1.1294 acc_val: 0.7066 time: 0.0034s\n",
      "Epoch: 113 loss_train: 1.1605 acc_train: 0.6626 loss_val: 1.1234 acc_val: 0.7066 time: 0.0036s\n",
      "Epoch: 114 loss_train: 1.1476 acc_train: 0.6773 loss_val: 1.1175 acc_val: 0.7085 time: 0.0032s\n",
      "Epoch: 115 loss_train: 1.1485 acc_train: 0.6730 loss_val: 1.1117 acc_val: 0.7048 time: 0.0031s\n",
      "Epoch: 116 loss_train: 1.1380 acc_train: 0.6798 loss_val: 1.1061 acc_val: 0.7085 time: 0.0032s\n",
      "Epoch: 117 loss_train: 1.1258 acc_train: 0.6810 loss_val: 1.1004 acc_val: 0.7159 time: 0.0031s\n",
      "Epoch: 118 loss_train: 1.1274 acc_train: 0.6749 loss_val: 1.0946 acc_val: 0.7159 time: 0.0031s\n",
      "Epoch: 119 loss_train: 1.1292 acc_train: 0.6656 loss_val: 1.0890 acc_val: 0.7159 time: 0.0034s\n",
      "Epoch: 120 loss_train: 1.1103 acc_train: 0.6761 loss_val: 1.0833 acc_val: 0.7159 time: 0.0034s\n",
      "Epoch: 121 loss_train: 1.1202 acc_train: 0.6736 loss_val: 1.0777 acc_val: 0.7196 time: 0.0034s\n",
      "Epoch: 122 loss_train: 1.1146 acc_train: 0.6792 loss_val: 1.0722 acc_val: 0.7251 time: 0.0031s\n",
      "Epoch: 123 loss_train: 1.1069 acc_train: 0.6798 loss_val: 1.0669 acc_val: 0.7306 time: 0.0031s\n",
      "Epoch: 124 loss_train: 1.1064 acc_train: 0.6872 loss_val: 1.0617 acc_val: 0.7325 time: 0.0031s\n",
      "Epoch: 125 loss_train: 1.0948 acc_train: 0.6958 loss_val: 1.0565 acc_val: 0.7399 time: 0.0031s\n",
      "Epoch: 126 loss_train: 1.1051 acc_train: 0.6952 loss_val: 1.0516 acc_val: 0.7417 time: 0.0031s\n",
      "Epoch: 127 loss_train: 1.0699 acc_train: 0.7087 loss_val: 1.0465 acc_val: 0.7362 time: 0.0031s\n",
      "Epoch: 128 loss_train: 1.0747 acc_train: 0.6909 loss_val: 1.0415 acc_val: 0.7362 time: 0.0034s\n",
      "Epoch: 129 loss_train: 1.0705 acc_train: 0.6977 loss_val: 1.0363 acc_val: 0.7362 time: 0.0034s\n",
      "Epoch: 130 loss_train: 1.0657 acc_train: 0.6946 loss_val: 1.0311 acc_val: 0.7399 time: 0.0036s\n",
      "Epoch: 131 loss_train: 1.0550 acc_train: 0.6964 loss_val: 1.0254 acc_val: 0.7435 time: 0.0034s\n",
      "Epoch: 132 loss_train: 1.0672 acc_train: 0.6995 loss_val: 1.0201 acc_val: 0.7491 time: 0.0031s\n",
      "Epoch: 133 loss_train: 1.0663 acc_train: 0.6903 loss_val: 1.0148 acc_val: 0.7546 time: 0.0036s\n",
      "Epoch: 134 loss_train: 1.0599 acc_train: 0.6958 loss_val: 1.0099 acc_val: 0.7546 time: 0.0038s\n",
      "Epoch: 135 loss_train: 1.0610 acc_train: 0.6964 loss_val: 1.0054 acc_val: 0.7583 time: 0.0033s\n",
      "Epoch: 136 loss_train: 1.0475 acc_train: 0.7124 loss_val: 1.0010 acc_val: 0.7583 time: 0.0032s\n",
      "Epoch: 137 loss_train: 1.0512 acc_train: 0.7149 loss_val: 0.9969 acc_val: 0.7601 time: 0.0033s\n",
      "Epoch: 138 loss_train: 1.0378 acc_train: 0.7069 loss_val: 0.9930 acc_val: 0.7620 time: 0.0032s\n",
      "Epoch: 139 loss_train: 1.0293 acc_train: 0.7100 loss_val: 0.9888 acc_val: 0.7657 time: 0.0031s\n",
      "Epoch: 140 loss_train: 1.0391 acc_train: 0.7309 loss_val: 0.9843 acc_val: 0.7657 time: 0.0032s\n",
      "Epoch: 141 loss_train: 1.0180 acc_train: 0.7118 loss_val: 0.9796 acc_val: 0.7749 time: 0.0035s\n",
      "Epoch: 142 loss_train: 1.0188 acc_train: 0.7180 loss_val: 0.9749 acc_val: 0.7749 time: 0.0034s\n",
      "Epoch: 143 loss_train: 1.0282 acc_train: 0.7149 loss_val: 0.9702 acc_val: 0.7786 time: 0.0036s\n",
      "Epoch: 144 loss_train: 1.0068 acc_train: 0.7328 loss_val: 0.9656 acc_val: 0.7841 time: 0.0032s\n",
      "Epoch: 145 loss_train: 1.0037 acc_train: 0.7266 loss_val: 0.9609 acc_val: 0.7841 time: 0.0033s\n",
      "Epoch: 146 loss_train: 0.9913 acc_train: 0.7383 loss_val: 0.9563 acc_val: 0.7860 time: 0.0032s\n",
      "Epoch: 147 loss_train: 0.9999 acc_train: 0.7414 loss_val: 0.9519 acc_val: 0.7878 time: 0.0031s\n",
      "Epoch: 148 loss_train: 0.9835 acc_train: 0.7500 loss_val: 0.9477 acc_val: 0.7860 time: 0.0032s\n",
      "Epoch: 149 loss_train: 0.9869 acc_train: 0.7377 loss_val: 0.9440 acc_val: 0.7841 time: 0.0033s\n",
      "Epoch: 150 loss_train: 0.9707 acc_train: 0.7438 loss_val: 0.9401 acc_val: 0.7841 time: 0.0031s\n",
      "Epoch: 151 loss_train: 0.9856 acc_train: 0.7432 loss_val: 0.9360 acc_val: 0.7860 time: 0.0031s\n",
      "Epoch: 152 loss_train: 0.9766 acc_train: 0.7426 loss_val: 0.9318 acc_val: 0.7934 time: 0.0034s\n",
      "Epoch: 153 loss_train: 0.9754 acc_train: 0.7383 loss_val: 0.9275 acc_val: 0.7970 time: 0.0034s\n",
      "Epoch: 154 loss_train: 0.9717 acc_train: 0.7395 loss_val: 0.9235 acc_val: 0.7989 time: 0.0033s\n",
      "Epoch: 155 loss_train: 0.9767 acc_train: 0.7445 loss_val: 0.9197 acc_val: 0.8007 time: 0.0031s\n",
      "Epoch: 156 loss_train: 0.9641 acc_train: 0.7525 loss_val: 0.9159 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 157 loss_train: 0.9555 acc_train: 0.7475 loss_val: 0.9123 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 158 loss_train: 0.9681 acc_train: 0.7494 loss_val: 0.9089 acc_val: 0.8007 time: 0.0031s\n",
      "Epoch: 159 loss_train: 0.9478 acc_train: 0.7605 loss_val: 0.9059 acc_val: 0.8007 time: 0.0033s\n",
      "Epoch: 160 loss_train: 0.9424 acc_train: 0.7648 loss_val: 0.9027 acc_val: 0.8007 time: 0.0033s\n",
      "Epoch: 161 loss_train: 0.9475 acc_train: 0.7562 loss_val: 0.8992 acc_val: 0.8007 time: 0.0034s\n",
      "Epoch: 162 loss_train: 0.9448 acc_train: 0.7494 loss_val: 0.8954 acc_val: 0.8026 time: 0.0032s\n",
      "Epoch: 163 loss_train: 0.9399 acc_train: 0.7685 loss_val: 0.8916 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 164 loss_train: 0.9318 acc_train: 0.7599 loss_val: 0.8879 acc_val: 0.8063 time: 0.0031s\n",
      "Epoch: 165 loss_train: 0.9370 acc_train: 0.7562 loss_val: 0.8846 acc_val: 0.8063 time: 0.0031s\n",
      "Epoch: 166 loss_train: 0.9142 acc_train: 0.7759 loss_val: 0.8813 acc_val: 0.8063 time: 0.0031s\n",
      "Epoch: 167 loss_train: 0.9330 acc_train: 0.7451 loss_val: 0.8784 acc_val: 0.8063 time: 0.0031s\n",
      "Epoch: 168 loss_train: 0.9333 acc_train: 0.7525 loss_val: 0.8758 acc_val: 0.8081 time: 0.0034s\n",
      "Epoch: 169 loss_train: 0.9382 acc_train: 0.7512 loss_val: 0.8734 acc_val: 0.8081 time: 0.0034s\n",
      "Epoch: 170 loss_train: 0.9184 acc_train: 0.7635 loss_val: 0.8713 acc_val: 0.8118 time: 0.0033s\n",
      "Epoch: 171 loss_train: 0.9003 acc_train: 0.7808 loss_val: 0.8686 acc_val: 0.8100 time: 0.0031s\n",
      "Epoch: 172 loss_train: 0.9155 acc_train: 0.7562 loss_val: 0.8657 acc_val: 0.8100 time: 0.0031s\n",
      "Epoch: 173 loss_train: 0.9059 acc_train: 0.7709 loss_val: 0.8622 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 174 loss_train: 0.9070 acc_train: 0.7642 loss_val: 0.8587 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 175 loss_train: 0.9013 acc_train: 0.7722 loss_val: 0.8554 acc_val: 0.8137 time: 0.0033s\n",
      "Epoch: 176 loss_train: 0.8977 acc_train: 0.7796 loss_val: 0.8521 acc_val: 0.8155 time: 0.0031s\n",
      "Epoch: 177 loss_train: 0.8994 acc_train: 0.7759 loss_val: 0.8489 acc_val: 0.8192 time: 0.0031s\n",
      "Epoch: 178 loss_train: 0.8843 acc_train: 0.7789 loss_val: 0.8461 acc_val: 0.8192 time: 0.0031s\n",
      "Epoch: 179 loss_train: 0.8899 acc_train: 0.7826 loss_val: 0.8435 acc_val: 0.8192 time: 0.0031s\n",
      "Epoch: 180 loss_train: 0.9004 acc_train: 0.7685 loss_val: 0.8409 acc_val: 0.8210 time: 0.0032s\n",
      "Epoch: 181 loss_train: 0.8758 acc_train: 0.7845 loss_val: 0.8384 acc_val: 0.8229 time: 0.0031s\n",
      "Epoch: 182 loss_train: 0.8902 acc_train: 0.7820 loss_val: 0.8357 acc_val: 0.8247 time: 0.0031s\n",
      "Epoch: 183 loss_train: 0.8825 acc_train: 0.7796 loss_val: 0.8332 acc_val: 0.8229 time: 0.0031s\n",
      "Epoch: 184 loss_train: 0.8820 acc_train: 0.7833 loss_val: 0.8309 acc_val: 0.8247 time: 0.0034s\n",
      "Epoch: 185 loss_train: 0.8623 acc_train: 0.7950 loss_val: 0.8283 acc_val: 0.8266 time: 0.0034s\n",
      "Epoch: 186 loss_train: 0.8777 acc_train: 0.7833 loss_val: 0.8258 acc_val: 0.8247 time: 0.0034s\n",
      "Epoch: 187 loss_train: 0.8613 acc_train: 0.7943 loss_val: 0.8230 acc_val: 0.8247 time: 0.0031s\n",
      "Epoch: 188 loss_train: 0.8771 acc_train: 0.7740 loss_val: 0.8203 acc_val: 0.8247 time: 0.0031s\n",
      "Epoch: 189 loss_train: 0.8663 acc_train: 0.7888 loss_val: 0.8177 acc_val: 0.8266 time: 0.0031s\n",
      "Epoch: 190 loss_train: 0.8673 acc_train: 0.7783 loss_val: 0.8153 acc_val: 0.8303 time: 0.0031s\n",
      "Epoch: 191 loss_train: 0.8686 acc_train: 0.7826 loss_val: 0.8131 acc_val: 0.8321 time: 0.0031s\n",
      "Epoch: 192 loss_train: 0.8653 acc_train: 0.7863 loss_val: 0.8110 acc_val: 0.8339 time: 0.0036s\n",
      "Epoch: 193 loss_train: 0.8633 acc_train: 0.7777 loss_val: 0.8090 acc_val: 0.8358 time: 0.0032s\n",
      "Epoch: 194 loss_train: 0.8608 acc_train: 0.7796 loss_val: 0.8070 acc_val: 0.8376 time: 0.0034s\n",
      "Epoch: 195 loss_train: 0.8524 acc_train: 0.7894 loss_val: 0.8052 acc_val: 0.8376 time: 0.0038s\n",
      "Epoch: 196 loss_train: 0.8490 acc_train: 0.7826 loss_val: 0.8037 acc_val: 0.8376 time: 0.0033s\n",
      "Epoch: 197 loss_train: 0.8442 acc_train: 0.7980 loss_val: 0.8017 acc_val: 0.8376 time: 0.0032s\n",
      "Epoch: 198 loss_train: 0.8596 acc_train: 0.7765 loss_val: 0.7992 acc_val: 0.8376 time: 0.0033s\n",
      "Epoch: 199 loss_train: 0.8366 acc_train: 0.7789 loss_val: 0.7965 acc_val: 0.8321 time: 0.0035s\n",
      "Epoch: 200 loss_train: 0.8262 acc_train: 0.7931 loss_val: 0.7936 acc_val: 0.8321 time: 0.0033s\n",
      "Test set results: loss= 0.8212 accuracy= 0.8173\n",
      "Training the model for the 5th time\n",
      "Epoch: 001 loss_train: 1.9384 acc_train: 0.1219 loss_val: 1.9319 acc_val: 0.2565 time: 0.0034s\n",
      "Epoch: 002 loss_train: 1.9331 acc_train: 0.1583 loss_val: 1.9268 acc_val: 0.1808 time: 0.0032s\n",
      "Epoch: 003 loss_train: 1.9286 acc_train: 0.1718 loss_val: 1.9213 acc_val: 0.1808 time: 0.0036s\n",
      "Epoch: 004 loss_train: 1.9230 acc_train: 0.1878 loss_val: 1.9154 acc_val: 0.1993 time: 0.0034s\n",
      "Epoch: 005 loss_train: 1.9175 acc_train: 0.2186 loss_val: 1.9093 acc_val: 0.2952 time: 0.0034s\n",
      "Epoch: 006 loss_train: 1.9103 acc_train: 0.2543 loss_val: 1.9028 acc_val: 0.3376 time: 0.0031s\n",
      "Epoch: 007 loss_train: 1.9049 acc_train: 0.2660 loss_val: 1.8958 acc_val: 0.3672 time: 0.0033s\n",
      "Epoch: 008 loss_train: 1.8976 acc_train: 0.2759 loss_val: 1.8887 acc_val: 0.3856 time: 0.0033s\n",
      "Epoch: 009 loss_train: 1.8894 acc_train: 0.3017 loss_val: 1.8812 acc_val: 0.3782 time: 0.0031s\n",
      "Epoch: 010 loss_train: 1.8820 acc_train: 0.2968 loss_val: 1.8736 acc_val: 0.3487 time: 0.0031s\n",
      "Epoch: 011 loss_train: 1.8754 acc_train: 0.3097 loss_val: 1.8659 acc_val: 0.3284 time: 0.0032s\n",
      "Epoch: 012 loss_train: 1.8659 acc_train: 0.3085 loss_val: 1.8580 acc_val: 0.3192 time: 0.0032s\n",
      "Epoch: 013 loss_train: 1.8596 acc_train: 0.3196 loss_val: 1.8502 acc_val: 0.3137 time: 0.0032s\n",
      "Epoch: 014 loss_train: 1.8542 acc_train: 0.2974 loss_val: 1.8423 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 015 loss_train: 1.8428 acc_train: 0.3023 loss_val: 1.8345 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 016 loss_train: 1.8357 acc_train: 0.3036 loss_val: 1.8269 acc_val: 0.3118 time: 0.0035s\n",
      "Epoch: 017 loss_train: 1.8282 acc_train: 0.2999 loss_val: 1.8194 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 018 loss_train: 1.8212 acc_train: 0.3079 loss_val: 1.8122 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 019 loss_train: 1.8125 acc_train: 0.3005 loss_val: 1.8053 acc_val: 0.3118 time: 0.0033s\n",
      "Epoch: 020 loss_train: 1.8000 acc_train: 0.2980 loss_val: 1.7987 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 021 loss_train: 1.8021 acc_train: 0.2986 loss_val: 1.7924 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 022 loss_train: 1.7963 acc_train: 0.2980 loss_val: 1.7864 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 023 loss_train: 1.7899 acc_train: 0.2956 loss_val: 1.7807 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 024 loss_train: 1.7794 acc_train: 0.2956 loss_val: 1.7752 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 025 loss_train: 1.7779 acc_train: 0.2943 loss_val: 1.7698 acc_val: 0.3118 time: 0.0032s\n",
      "Epoch: 026 loss_train: 1.7752 acc_train: 0.2956 loss_val: 1.7645 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 027 loss_train: 1.7677 acc_train: 0.2956 loss_val: 1.7592 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 028 loss_train: 1.7658 acc_train: 0.2950 loss_val: 1.7537 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 029 loss_train: 1.7587 acc_train: 0.2950 loss_val: 1.7480 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 030 loss_train: 1.7519 acc_train: 0.2956 loss_val: 1.7421 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 031 loss_train: 1.7458 acc_train: 0.2956 loss_val: 1.7359 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 032 loss_train: 1.7356 acc_train: 0.2956 loss_val: 1.7294 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 033 loss_train: 1.7258 acc_train: 0.2950 loss_val: 1.7228 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 034 loss_train: 1.7269 acc_train: 0.2950 loss_val: 1.7159 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 035 loss_train: 1.7218 acc_train: 0.2956 loss_val: 1.7088 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 036 loss_train: 1.7116 acc_train: 0.2956 loss_val: 1.7017 acc_val: 0.3118 time: 0.0034s\n",
      "Epoch: 037 loss_train: 1.7043 acc_train: 0.2974 loss_val: 1.6946 acc_val: 0.3118 time: 0.0031s\n",
      "Epoch: 038 loss_train: 1.6981 acc_train: 0.2956 loss_val: 1.6875 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 039 loss_train: 1.6969 acc_train: 0.2986 loss_val: 1.6804 acc_val: 0.3137 time: 0.0033s\n",
      "Epoch: 040 loss_train: 1.6842 acc_train: 0.2999 loss_val: 1.6732 acc_val: 0.3137 time: 0.0033s\n",
      "Epoch: 041 loss_train: 1.6786 acc_train: 0.2999 loss_val: 1.6659 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 042 loss_train: 1.6691 acc_train: 0.3079 loss_val: 1.6584 acc_val: 0.3137 time: 0.0031s\n",
      "Epoch: 043 loss_train: 1.6612 acc_train: 0.3079 loss_val: 1.6509 acc_val: 0.3173 time: 0.0031s\n",
      "Epoch: 044 loss_train: 1.6528 acc_train: 0.3153 loss_val: 1.6432 acc_val: 0.3173 time: 0.0032s\n",
      "Epoch: 045 loss_train: 1.6490 acc_train: 0.3239 loss_val: 1.6355 acc_val: 0.3210 time: 0.0031s\n",
      "Epoch: 046 loss_train: 1.6411 acc_train: 0.3245 loss_val: 1.6277 acc_val: 0.3247 time: 0.0031s\n",
      "Epoch: 047 loss_train: 1.6358 acc_train: 0.3233 loss_val: 1.6199 acc_val: 0.3284 time: 0.0031s\n",
      "Epoch: 048 loss_train: 1.6260 acc_train: 0.3350 loss_val: 1.6121 acc_val: 0.3321 time: 0.0034s\n",
      "Epoch: 049 loss_train: 1.6115 acc_train: 0.3362 loss_val: 1.6041 acc_val: 0.3358 time: 0.0034s\n",
      "Epoch: 050 loss_train: 1.6093 acc_train: 0.3424 loss_val: 1.5961 acc_val: 0.3395 time: 0.0034s\n",
      "Epoch: 051 loss_train: 1.6043 acc_train: 0.3504 loss_val: 1.5881 acc_val: 0.3395 time: 0.0031s\n",
      "Epoch: 052 loss_train: 1.5896 acc_train: 0.3541 loss_val: 1.5800 acc_val: 0.3413 time: 0.0031s\n",
      "Epoch: 053 loss_train: 1.5790 acc_train: 0.3534 loss_val: 1.5719 acc_val: 0.3469 time: 0.0031s\n",
      "Epoch: 054 loss_train: 1.5761 acc_train: 0.3695 loss_val: 1.5637 acc_val: 0.3579 time: 0.0031s\n",
      "Epoch: 055 loss_train: 1.5689 acc_train: 0.3682 loss_val: 1.5555 acc_val: 0.3653 time: 0.0031s\n",
      "Epoch: 056 loss_train: 1.5568 acc_train: 0.3744 loss_val: 1.5474 acc_val: 0.3745 time: 0.0038s\n",
      "Epoch: 057 loss_train: 1.5506 acc_train: 0.3793 loss_val: 1.5393 acc_val: 0.3782 time: 0.0036s\n",
      "Epoch: 058 loss_train: 1.5448 acc_train: 0.3787 loss_val: 1.5311 acc_val: 0.3819 time: 0.0031s\n",
      "Epoch: 059 loss_train: 1.5355 acc_train: 0.3935 loss_val: 1.5229 acc_val: 0.3856 time: 0.0032s\n",
      "Epoch: 060 loss_train: 1.5231 acc_train: 0.4150 loss_val: 1.5148 acc_val: 0.3948 time: 0.0034s\n",
      "Epoch: 061 loss_train: 1.5193 acc_train: 0.4156 loss_val: 1.5066 acc_val: 0.4022 time: 0.0033s\n",
      "Epoch: 062 loss_train: 1.5140 acc_train: 0.4076 loss_val: 1.4985 acc_val: 0.4114 time: 0.0032s\n",
      "Epoch: 063 loss_train: 1.5049 acc_train: 0.4273 loss_val: 1.4906 acc_val: 0.4225 time: 0.0033s\n",
      "Epoch: 064 loss_train: 1.4920 acc_train: 0.4310 loss_val: 1.4828 acc_val: 0.4280 time: 0.0035s\n",
      "Epoch: 065 loss_train: 1.4806 acc_train: 0.4452 loss_val: 1.4751 acc_val: 0.4299 time: 0.0034s\n",
      "Epoch: 066 loss_train: 1.4745 acc_train: 0.4532 loss_val: 1.4674 acc_val: 0.4410 time: 0.0031s\n",
      "Epoch: 067 loss_train: 1.4693 acc_train: 0.4483 loss_val: 1.4596 acc_val: 0.4410 time: 0.0031s\n",
      "Epoch: 068 loss_train: 1.4673 acc_train: 0.4477 loss_val: 1.4519 acc_val: 0.4483 time: 0.0031s\n",
      "Epoch: 069 loss_train: 1.4518 acc_train: 0.4686 loss_val: 1.4442 acc_val: 0.4520 time: 0.0031s\n",
      "Epoch: 070 loss_train: 1.4455 acc_train: 0.4637 loss_val: 1.4366 acc_val: 0.4539 time: 0.0031s\n",
      "Epoch: 071 loss_train: 1.4307 acc_train: 0.4778 loss_val: 1.4290 acc_val: 0.4557 time: 0.0031s\n",
      "Epoch: 072 loss_train: 1.4291 acc_train: 0.4692 loss_val: 1.4216 acc_val: 0.4668 time: 0.0034s\n",
      "Epoch: 073 loss_train: 1.4278 acc_train: 0.4803 loss_val: 1.4144 acc_val: 0.4705 time: 0.0031s\n",
      "Epoch: 074 loss_train: 1.4243 acc_train: 0.4667 loss_val: 1.4071 acc_val: 0.4742 time: 0.0035s\n",
      "Epoch: 075 loss_train: 1.4057 acc_train: 0.4877 loss_val: 1.3999 acc_val: 0.4760 time: 0.0034s\n",
      "Epoch: 076 loss_train: 1.3975 acc_train: 0.5000 loss_val: 1.3927 acc_val: 0.4834 time: 0.0034s\n",
      "Epoch: 077 loss_train: 1.3957 acc_train: 0.5099 loss_val: 1.3854 acc_val: 0.4871 time: 0.0031s\n",
      "Epoch: 078 loss_train: 1.3872 acc_train: 0.4988 loss_val: 1.3783 acc_val: 0.4945 time: 0.0032s\n",
      "Epoch: 079 loss_train: 1.3899 acc_train: 0.5062 loss_val: 1.3712 acc_val: 0.5037 time: 0.0031s\n",
      "Epoch: 080 loss_train: 1.3729 acc_train: 0.5135 loss_val: 1.3643 acc_val: 0.5074 time: 0.0032s\n",
      "Epoch: 081 loss_train: 1.3692 acc_train: 0.5216 loss_val: 1.3574 acc_val: 0.5074 time: 0.0031s\n",
      "Epoch: 082 loss_train: 1.3607 acc_train: 0.5216 loss_val: 1.3506 acc_val: 0.5074 time: 0.0031s\n",
      "Epoch: 083 loss_train: 1.3587 acc_train: 0.5271 loss_val: 1.3438 acc_val: 0.5092 time: 0.0034s\n",
      "Epoch: 084 loss_train: 1.3497 acc_train: 0.5302 loss_val: 1.3370 acc_val: 0.5111 time: 0.0035s\n",
      "Epoch: 085 loss_train: 1.3442 acc_train: 0.5339 loss_val: 1.3304 acc_val: 0.5185 time: 0.0036s\n",
      "Epoch: 086 loss_train: 1.3375 acc_train: 0.5394 loss_val: 1.3237 acc_val: 0.5203 time: 0.0032s\n",
      "Epoch: 087 loss_train: 1.3318 acc_train: 0.5425 loss_val: 1.3171 acc_val: 0.5258 time: 0.0031s\n",
      "Epoch: 088 loss_train: 1.3299 acc_train: 0.5289 loss_val: 1.3106 acc_val: 0.5295 time: 0.0032s\n",
      "Epoch: 089 loss_train: 1.3070 acc_train: 0.5511 loss_val: 1.3041 acc_val: 0.5351 time: 0.0032s\n",
      "Epoch: 090 loss_train: 1.3116 acc_train: 0.5462 loss_val: 1.2977 acc_val: 0.5443 time: 0.0033s\n",
      "Epoch: 091 loss_train: 1.2979 acc_train: 0.5591 loss_val: 1.2914 acc_val: 0.5461 time: 0.0032s\n",
      "Epoch: 092 loss_train: 1.2989 acc_train: 0.5603 loss_val: 1.2852 acc_val: 0.5461 time: 0.0032s\n",
      "Epoch: 093 loss_train: 1.2880 acc_train: 0.5560 loss_val: 1.2789 acc_val: 0.5480 time: 0.0032s\n",
      "Epoch: 094 loss_train: 1.2848 acc_train: 0.5770 loss_val: 1.2727 acc_val: 0.5535 time: 0.0032s\n",
      "Epoch: 095 loss_train: 1.2865 acc_train: 0.5807 loss_val: 1.2665 acc_val: 0.5627 time: 0.0032s\n",
      "Epoch: 096 loss_train: 1.2759 acc_train: 0.5751 loss_val: 1.2602 acc_val: 0.5646 time: 0.0033s\n",
      "Epoch: 097 loss_train: 1.2705 acc_train: 0.5930 loss_val: 1.2539 acc_val: 0.5683 time: 0.0035s\n",
      "Epoch: 098 loss_train: 1.2637 acc_train: 0.5930 loss_val: 1.2476 acc_val: 0.5775 time: 0.0034s\n",
      "Epoch: 099 loss_train: 1.2566 acc_train: 0.5979 loss_val: 1.2413 acc_val: 0.5849 time: 0.0032s\n",
      "Epoch: 100 loss_train: 1.2417 acc_train: 0.6078 loss_val: 1.2350 acc_val: 0.5959 time: 0.0032s\n",
      "Epoch: 101 loss_train: 1.2423 acc_train: 0.5973 loss_val: 1.2286 acc_val: 0.5978 time: 0.0032s\n",
      "Epoch: 102 loss_train: 1.2388 acc_train: 0.6084 loss_val: 1.2222 acc_val: 0.6144 time: 0.0033s\n",
      "Epoch: 103 loss_train: 1.2273 acc_train: 0.6102 loss_val: 1.2158 acc_val: 0.6199 time: 0.0035s\n",
      "Epoch: 104 loss_train: 1.2212 acc_train: 0.6250 loss_val: 1.2094 acc_val: 0.6310 time: 0.0032s\n",
      "Epoch: 105 loss_train: 1.2172 acc_train: 0.6318 loss_val: 1.2032 acc_val: 0.6421 time: 0.0031s\n",
      "Epoch: 106 loss_train: 1.2199 acc_train: 0.6244 loss_val: 1.1970 acc_val: 0.6458 time: 0.0035s\n",
      "Epoch: 107 loss_train: 1.2086 acc_train: 0.6472 loss_val: 1.1908 acc_val: 0.6513 time: 0.0035s\n",
      "Epoch: 108 loss_train: 1.1991 acc_train: 0.6472 loss_val: 1.1843 acc_val: 0.6587 time: 0.0034s\n",
      "Epoch: 109 loss_train: 1.1991 acc_train: 0.6410 loss_val: 1.1777 acc_val: 0.6624 time: 0.0032s\n",
      "Epoch: 110 loss_train: 1.1921 acc_train: 0.6656 loss_val: 1.1711 acc_val: 0.6661 time: 0.0032s\n",
      "Epoch: 111 loss_train: 1.1802 acc_train: 0.6626 loss_val: 1.1644 acc_val: 0.6753 time: 0.0032s\n",
      "Epoch: 112 loss_train: 1.1814 acc_train: 0.6626 loss_val: 1.1578 acc_val: 0.6827 time: 0.0032s\n",
      "Epoch: 113 loss_train: 1.1668 acc_train: 0.6681 loss_val: 1.1513 acc_val: 0.6827 time: 0.0033s\n",
      "Epoch: 114 loss_train: 1.1593 acc_train: 0.6773 loss_val: 1.1448 acc_val: 0.6863 time: 0.0033s\n",
      "Epoch: 115 loss_train: 1.1576 acc_train: 0.6656 loss_val: 1.1383 acc_val: 0.6919 time: 0.0031s\n",
      "Epoch: 116 loss_train: 1.1572 acc_train: 0.6736 loss_val: 1.1318 acc_val: 0.6993 time: 0.0034s\n",
      "Epoch: 117 loss_train: 1.1464 acc_train: 0.6780 loss_val: 1.1254 acc_val: 0.7048 time: 0.0037s\n",
      "Epoch: 118 loss_train: 1.1477 acc_train: 0.6773 loss_val: 1.1191 acc_val: 0.7085 time: 0.0035s\n",
      "Epoch: 119 loss_train: 1.1259 acc_train: 0.6853 loss_val: 1.1127 acc_val: 0.7103 time: 0.0032s\n",
      "Epoch: 120 loss_train: 1.1291 acc_train: 0.6977 loss_val: 1.1062 acc_val: 0.7103 time: 0.0032s\n",
      "Epoch: 121 loss_train: 1.1195 acc_train: 0.7094 loss_val: 1.0996 acc_val: 0.7122 time: 0.0033s\n",
      "Epoch: 122 loss_train: 1.1252 acc_train: 0.6897 loss_val: 1.0932 acc_val: 0.7140 time: 0.0031s\n",
      "Epoch: 123 loss_train: 1.1081 acc_train: 0.6983 loss_val: 1.0867 acc_val: 0.7232 time: 0.0031s\n",
      "Epoch: 124 loss_train: 1.1107 acc_train: 0.7063 loss_val: 1.0803 acc_val: 0.7269 time: 0.0034s\n",
      "Epoch: 125 loss_train: 1.1026 acc_train: 0.7063 loss_val: 1.0739 acc_val: 0.7306 time: 0.0033s\n",
      "Epoch: 126 loss_train: 1.0939 acc_train: 0.7063 loss_val: 1.0674 acc_val: 0.7362 time: 0.0035s\n",
      "Epoch: 127 loss_train: 1.0811 acc_train: 0.7038 loss_val: 1.0610 acc_val: 0.7380 time: 0.0033s\n",
      "Epoch: 128 loss_train: 1.0770 acc_train: 0.7087 loss_val: 1.0545 acc_val: 0.7417 time: 0.0032s\n",
      "Epoch: 129 loss_train: 1.0829 acc_train: 0.7180 loss_val: 1.0481 acc_val: 0.7435 time: 0.0031s\n",
      "Epoch: 130 loss_train: 1.0814 acc_train: 0.7094 loss_val: 1.0420 acc_val: 0.7435 time: 0.0032s\n",
      "Epoch: 131 loss_train: 1.0759 acc_train: 0.7112 loss_val: 1.0361 acc_val: 0.7435 time: 0.0032s\n",
      "Epoch: 132 loss_train: 1.0507 acc_train: 0.7198 loss_val: 1.0303 acc_val: 0.7472 time: 0.0033s\n",
      "Epoch: 133 loss_train: 1.0548 acc_train: 0.7297 loss_val: 1.0245 acc_val: 0.7546 time: 0.0033s\n",
      "Epoch: 134 loss_train: 1.0531 acc_train: 0.7229 loss_val: 1.0189 acc_val: 0.7546 time: 0.0034s\n",
      "Epoch: 135 loss_train: 1.0472 acc_train: 0.7328 loss_val: 1.0133 acc_val: 0.7546 time: 0.0035s\n",
      "Epoch: 136 loss_train: 1.0382 acc_train: 0.7192 loss_val: 1.0078 acc_val: 0.7546 time: 0.0032s\n",
      "Epoch: 137 loss_train: 1.0372 acc_train: 0.7278 loss_val: 1.0023 acc_val: 0.7601 time: 0.0031s\n",
      "Epoch: 138 loss_train: 1.0376 acc_train: 0.7352 loss_val: 0.9971 acc_val: 0.7583 time: 0.0031s\n",
      "Epoch: 139 loss_train: 1.0366 acc_train: 0.7334 loss_val: 0.9919 acc_val: 0.7583 time: 0.0034s\n",
      "Epoch: 140 loss_train: 1.0320 acc_train: 0.7315 loss_val: 0.9870 acc_val: 0.7583 time: 0.0031s\n",
      "Epoch: 141 loss_train: 1.0160 acc_train: 0.7506 loss_val: 0.9820 acc_val: 0.7601 time: 0.0031s\n",
      "Epoch: 142 loss_train: 1.0144 acc_train: 0.7482 loss_val: 0.9770 acc_val: 0.7601 time: 0.0032s\n",
      "Epoch: 143 loss_train: 1.0155 acc_train: 0.7383 loss_val: 0.9722 acc_val: 0.7601 time: 0.0034s\n",
      "Epoch: 144 loss_train: 0.9980 acc_train: 0.7549 loss_val: 0.9675 acc_val: 0.7601 time: 0.0033s\n",
      "Epoch: 145 loss_train: 0.9976 acc_train: 0.7432 loss_val: 0.9629 acc_val: 0.7601 time: 0.0031s\n",
      "Epoch: 146 loss_train: 1.0039 acc_train: 0.7500 loss_val: 0.9584 acc_val: 0.7601 time: 0.0031s\n",
      "Epoch: 147 loss_train: 0.9968 acc_train: 0.7494 loss_val: 0.9538 acc_val: 0.7657 time: 0.0034s\n",
      "Epoch: 148 loss_train: 0.9919 acc_train: 0.7482 loss_val: 0.9494 acc_val: 0.7675 time: 0.0031s\n",
      "Epoch: 149 loss_train: 0.9954 acc_train: 0.7426 loss_val: 0.9454 acc_val: 0.7694 time: 0.0032s\n",
      "Epoch: 150 loss_train: 0.9788 acc_train: 0.7494 loss_val: 0.9415 acc_val: 0.7712 time: 0.0031s\n",
      "Epoch: 151 loss_train: 0.9957 acc_train: 0.7451 loss_val: 0.9376 acc_val: 0.7749 time: 0.0034s\n",
      "Epoch: 152 loss_train: 0.9691 acc_train: 0.7555 loss_val: 0.9338 acc_val: 0.7749 time: 0.0034s\n",
      "Epoch: 153 loss_train: 0.9719 acc_train: 0.7562 loss_val: 0.9301 acc_val: 0.7768 time: 0.0031s\n",
      "Epoch: 154 loss_train: 0.9704 acc_train: 0.7599 loss_val: 0.9265 acc_val: 0.7768 time: 0.0033s\n",
      "Epoch: 155 loss_train: 0.9570 acc_train: 0.7599 loss_val: 0.9230 acc_val: 0.7804 time: 0.0033s\n",
      "Epoch: 156 loss_train: 0.9608 acc_train: 0.7666 loss_val: 0.9194 acc_val: 0.7786 time: 0.0032s\n",
      "Epoch: 157 loss_train: 0.9485 acc_train: 0.7666 loss_val: 0.9156 acc_val: 0.7786 time: 0.0031s\n",
      "Epoch: 158 loss_train: 0.9595 acc_train: 0.7629 loss_val: 0.9121 acc_val: 0.7804 time: 0.0031s\n",
      "Epoch: 159 loss_train: 0.9550 acc_train: 0.7543 loss_val: 0.9084 acc_val: 0.7823 time: 0.0032s\n",
      "Epoch: 160 loss_train: 0.9441 acc_train: 0.7617 loss_val: 0.9048 acc_val: 0.7860 time: 0.0031s\n",
      "Epoch: 161 loss_train: 0.9349 acc_train: 0.7777 loss_val: 0.9012 acc_val: 0.7878 time: 0.0031s\n",
      "Epoch: 162 loss_train: 0.9517 acc_train: 0.7666 loss_val: 0.8979 acc_val: 0.7860 time: 0.0034s\n",
      "Epoch: 163 loss_train: 0.9445 acc_train: 0.7635 loss_val: 0.8944 acc_val: 0.7915 time: 0.0031s\n",
      "Epoch: 164 loss_train: 0.9337 acc_train: 0.7697 loss_val: 0.8909 acc_val: 0.7934 time: 0.0031s\n",
      "Epoch: 165 loss_train: 0.9313 acc_train: 0.7703 loss_val: 0.8874 acc_val: 0.7970 time: 0.0033s\n",
      "Epoch: 166 loss_train: 0.9377 acc_train: 0.7808 loss_val: 0.8838 acc_val: 0.8007 time: 0.0031s\n",
      "Epoch: 167 loss_train: 0.9273 acc_train: 0.7752 loss_val: 0.8802 acc_val: 0.8007 time: 0.0034s\n",
      "Epoch: 168 loss_train: 0.9044 acc_train: 0.7765 loss_val: 0.8767 acc_val: 0.8026 time: 0.0034s\n",
      "Epoch: 169 loss_train: 0.9214 acc_train: 0.7752 loss_val: 0.8733 acc_val: 0.8044 time: 0.0031s\n",
      "Epoch: 170 loss_train: 0.9130 acc_train: 0.7746 loss_val: 0.8701 acc_val: 0.8026 time: 0.0031s\n",
      "Epoch: 171 loss_train: 0.9074 acc_train: 0.7771 loss_val: 0.8670 acc_val: 0.8007 time: 0.0031s\n",
      "Epoch: 172 loss_train: 0.8887 acc_train: 0.7919 loss_val: 0.8637 acc_val: 0.8044 time: 0.0031s\n",
      "Epoch: 173 loss_train: 0.9046 acc_train: 0.7833 loss_val: 0.8603 acc_val: 0.8081 time: 0.0031s\n",
      "Epoch: 174 loss_train: 0.9004 acc_train: 0.7808 loss_val: 0.8570 acc_val: 0.8100 time: 0.0031s\n",
      "Epoch: 175 loss_train: 0.8768 acc_train: 0.7869 loss_val: 0.8533 acc_val: 0.8118 time: 0.0032s\n",
      "Epoch: 176 loss_train: 0.8918 acc_train: 0.7919 loss_val: 0.8493 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 177 loss_train: 0.8838 acc_train: 0.7919 loss_val: 0.8457 acc_val: 0.8137 time: 0.0031s\n",
      "Epoch: 178 loss_train: 0.8804 acc_train: 0.7869 loss_val: 0.8421 acc_val: 0.8155 time: 0.0031s\n",
      "Epoch: 179 loss_train: 0.8699 acc_train: 0.7950 loss_val: 0.8388 acc_val: 0.8173 time: 0.0039s\n",
      "Epoch: 180 loss_train: 0.8856 acc_train: 0.7851 loss_val: 0.8359 acc_val: 0.8173 time: 0.0035s\n",
      "Epoch: 181 loss_train: 0.8828 acc_train: 0.7900 loss_val: 0.8334 acc_val: 0.8173 time: 0.0031s\n",
      "Epoch: 182 loss_train: 0.8802 acc_train: 0.7888 loss_val: 0.8312 acc_val: 0.8173 time: 0.0031s\n",
      "Epoch: 183 loss_train: 0.8601 acc_train: 0.7968 loss_val: 0.8289 acc_val: 0.8192 time: 0.0033s\n",
      "Epoch: 184 loss_train: 0.8681 acc_train: 0.8042 loss_val: 0.8260 acc_val: 0.8229 time: 0.0034s\n",
      "Epoch: 185 loss_train: 0.8704 acc_train: 0.7882 loss_val: 0.8229 acc_val: 0.8247 time: 0.0033s\n",
      "Epoch: 186 loss_train: 0.8530 acc_train: 0.8017 loss_val: 0.8200 acc_val: 0.8247 time: 0.0034s\n",
      "Epoch: 187 loss_train: 0.8605 acc_train: 0.8048 loss_val: 0.8170 acc_val: 0.8247 time: 0.0037s\n",
      "Epoch: 188 loss_train: 0.8521 acc_train: 0.7956 loss_val: 0.8142 acc_val: 0.8247 time: 0.0036s\n",
      "Epoch: 189 loss_train: 0.8695 acc_train: 0.7986 loss_val: 0.8115 acc_val: 0.8266 time: 0.0035s\n",
      "Epoch: 190 loss_train: 0.8531 acc_train: 0.7993 loss_val: 0.8090 acc_val: 0.8229 time: 0.0034s\n",
      "Epoch: 191 loss_train: 0.8476 acc_train: 0.7993 loss_val: 0.8066 acc_val: 0.8229 time: 0.0033s\n",
      "Epoch: 192 loss_train: 0.8474 acc_train: 0.7931 loss_val: 0.8044 acc_val: 0.8210 time: 0.0033s\n",
      "Epoch: 193 loss_train: 0.8541 acc_train: 0.8005 loss_val: 0.8024 acc_val: 0.8229 time: 0.0033s\n",
      "Epoch: 194 loss_train: 0.8462 acc_train: 0.7900 loss_val: 0.8000 acc_val: 0.8284 time: 0.0035s\n",
      "Epoch: 195 loss_train: 0.8362 acc_train: 0.8036 loss_val: 0.7975 acc_val: 0.8284 time: 0.0036s\n",
      "Epoch: 196 loss_train: 0.8444 acc_train: 0.8171 loss_val: 0.7950 acc_val: 0.8303 time: 0.0035s\n",
      "Epoch: 197 loss_train: 0.8413 acc_train: 0.7956 loss_val: 0.7927 acc_val: 0.8339 time: 0.0036s\n",
      "Epoch: 198 loss_train: 0.8413 acc_train: 0.8036 loss_val: 0.7906 acc_val: 0.8321 time: 0.0035s\n",
      "Epoch: 199 loss_train: 0.8441 acc_train: 0.8110 loss_val: 0.7886 acc_val: 0.8303 time: 0.0038s\n",
      "Epoch: 200 loss_train: 0.8363 acc_train: 0.8208 loss_val: 0.7869 acc_val: 0.8303 time: 0.0034s\n",
      "Test set results: loss= 0.8216 accuracy= 0.8210\n"
     ]
    }
   ],
   "source": [
    "loss_test = []\n",
    "acc_test = []\n",
    "for i in range(5):\n",
    "    print(\"Training the model for the {}th time\".format(i+1))\n",
    "    model = GNN(features.shape[1], n_hidden, n_class, dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch, model)\n",
    "    loss, acc = test()\n",
    "    loss_test.append(loss)\n",
    "    acc_test.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8210\n",
      "Standard deviation: 0.0100\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy: {:.4f}\".format(np.mean(acc_test)))\n",
    "print(\"Standard deviation: {:.4f}\".format(np.std(acc_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
