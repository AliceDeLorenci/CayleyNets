{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN for graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from random import randint\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.sparse import diags, eye\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import time\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    # Ãƒ = A + I\n",
    "    A_tilde = A + sp.identity(A.shape[0])\n",
    "\n",
    "    # degree matrix D\n",
    "    D = np.sum(A_tilde, axis=1)\n",
    "    D_inv_sqrt = 1 / np.sqrt(D)\n",
    "    D_inv_sqrt = np.squeeze(np.asarray(D_inv_sqrt))\n",
    "    D_inv_sqrt_matrix = sp.diags(D_inv_sqrt, format='csc')\n",
    "\n",
    "    # Normalized adjacency matrix A_normalized\n",
    "    A_normalized = D_inv_sqrt_matrix @ A_tilde @ D_inv_sqrt_matrix\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "\n",
    "def load_cora():\n",
    "    idx_features_labels = np.genfromtxt(\"./data/cora.content\", dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    features = features.todense()\n",
    "    features /= features.sum(1).reshape(-1, 1)\n",
    "    \n",
    "    class_labels = idx_features_labels[:, -1]\n",
    "    le = LabelEncoder()\n",
    "    class_labels = le.fit_transform(class_labels)\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"./data/cora.cites\", dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(class_labels.size, class_labels.size), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    return features, adj, class_labels\n",
    "\n",
    "\n",
    "def sparse_to_torch_sparse(M):\n",
    "    \"\"\"Converts a sparse SciPy matrix to a sparse PyTorch tensor\"\"\"\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((M.row, M.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\" taken from LAB 5 on graph neural networks of the cours ALTEGRAD from MVA 2023 \n",
    "        Lecture: Prof. Michalis Vazirgiannis\n",
    "        Lab: Dr. Giannis Nikolentzos & Dr. Johannes Lutzeyer \"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, n_class, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, n_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj):\n",
    "        \n",
    "        # First layer\n",
    "        z1 = self.fc1(x_in)\n",
    "        h1 = self.relu(torch.mm(adj, z1))\n",
    "        h1 = self.dropout(h1)\n",
    "\n",
    "        # Second layer\n",
    "        z2 = self.fc2(h1)\n",
    "        h2 = self.relu(torch.mm(adj, z2))\n",
    "       \n",
    "        # Output layer\n",
    "        x = self.fc3(h2)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), h2 #, h2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 32\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Read data\n",
    "features, adj, class_labels = load_cora()\n",
    "n = adj.shape[0] # Number of nodes\n",
    "n_class = np.unique(class_labels).size # Number of classes\n",
    "\n",
    "adj = normalize_adjacency(adj) # Normalize adjacency matrix\n",
    "\n",
    "# Yields indices to split data into training, validation and test sets\n",
    "idx = np.random.permutation(n)\n",
    "idx_train = idx[:int(0.6*n)]\n",
    "idx_val = idx[int(0.6*n):int(0.8*n)]\n",
    "idx_test = idx[int(0.8*n):]\n",
    "\n",
    "# Transform the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features).to(device)\n",
    "y = torch.LongTensor(class_labels).to(device)\n",
    "adj = sparse_to_torch_sparse(adj).to(device)\n",
    "idx_train = torch.LongTensor(idx_train).to(device)\n",
    "idx_val = torch.LongTensor(idx_val).to(device)\n",
    "idx_test = torch.LongTensor(idx_test).to(device)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GNN(features.shape[1], n_hidden_1, n_hidden_2, n_class, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output,_ = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], y[idx_train])\n",
    "    acc_train = accuracy_score(torch.argmax(output[idx_train], dim=1).detach().cpu().numpy(), y[idx_train].cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    output,_ = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], y[idx_val])\n",
    "    acc_val = accuracy_score(torch.argmax(output[idx_val], dim=1).detach().cpu().numpy(), y[idx_val].cpu().numpy())\n",
    "    print('Epoch: {:03d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output, embeddings = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], y[idx_test])\n",
    "    acc_test = accuracy_score(torch.argmax(output[idx_test], dim=1).detach().cpu().numpy(), y[idx_test].cpu().numpy())\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "\n",
    "    return embeddings[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 loss_train: 1.9475 acc_train: 0.0764 loss_val: 1.9420 acc_val: 0.0978 time: 0.5946s\n",
      "Epoch: 002 loss_train: 1.9385 acc_train: 0.0770 loss_val: 1.9350 acc_val: 0.0996 time: 0.5192s\n",
      "Epoch: 003 loss_train: 1.9303 acc_train: 0.0850 loss_val: 1.9287 acc_val: 0.3007 time: 0.5021s\n",
      "Epoch: 004 loss_train: 1.9226 acc_train: 0.3073 loss_val: 1.9210 acc_val: 0.2288 time: 0.5750s\n",
      "Epoch: 005 loss_train: 1.9142 acc_train: 0.2248 loss_val: 1.9120 acc_val: 0.2454 time: 0.5025s\n",
      "Epoch: 006 loss_train: 1.9034 acc_train: 0.2580 loss_val: 1.9020 acc_val: 0.3247 time: 0.5190s\n",
      "Epoch: 007 loss_train: 1.8925 acc_train: 0.3140 loss_val: 1.8910 acc_val: 0.3063 time: 0.5035s\n",
      "Epoch: 008 loss_train: 1.8791 acc_train: 0.3528 loss_val: 1.8794 acc_val: 0.2768 time: 0.5326s\n",
      "Epoch: 009 loss_train: 1.8647 acc_train: 0.3140 loss_val: 1.8669 acc_val: 0.2768 time: 0.4788s\n",
      "Epoch: 010 loss_train: 1.8484 acc_train: 0.3073 loss_val: 1.8542 acc_val: 0.2731 time: 0.4933s\n",
      "Epoch: 011 loss_train: 1.8325 acc_train: 0.3067 loss_val: 1.8421 acc_val: 0.2731 time: 0.4653s\n",
      "Epoch: 012 loss_train: 1.8146 acc_train: 0.3054 loss_val: 1.8321 acc_val: 0.2731 time: 0.4862s\n",
      "Epoch: 013 loss_train: 1.7966 acc_train: 0.3054 loss_val: 1.8252 acc_val: 0.2731 time: 0.4990s\n",
      "Epoch: 014 loss_train: 1.7863 acc_train: 0.3054 loss_val: 1.8212 acc_val: 0.2731 time: 0.5014s\n",
      "Epoch: 015 loss_train: 1.7730 acc_train: 0.3054 loss_val: 1.8180 acc_val: 0.2731 time: 0.4897s\n",
      "Epoch: 016 loss_train: 1.7689 acc_train: 0.3054 loss_val: 1.8115 acc_val: 0.2731 time: 0.5323s\n",
      "Epoch: 017 loss_train: 1.7531 acc_train: 0.3054 loss_val: 1.7984 acc_val: 0.2731 time: 0.5093s\n",
      "Epoch: 018 loss_train: 1.7343 acc_train: 0.3054 loss_val: 1.7801 acc_val: 0.2731 time: 0.5015s\n",
      "Epoch: 019 loss_train: 1.7187 acc_train: 0.3054 loss_val: 1.7592 acc_val: 0.2731 time: 0.5152s\n",
      "Epoch: 020 loss_train: 1.6992 acc_train: 0.3054 loss_val: 1.7361 acc_val: 0.2731 time: 0.5314s\n",
      "Epoch: 021 loss_train: 1.6761 acc_train: 0.3054 loss_val: 1.7106 acc_val: 0.2731 time: 0.4602s\n",
      "Epoch: 022 loss_train: 1.6517 acc_train: 0.3054 loss_val: 1.6833 acc_val: 0.2731 time: 0.5171s\n",
      "Epoch: 023 loss_train: 1.6279 acc_train: 0.3054 loss_val: 1.6542 acc_val: 0.2731 time: 0.5156s\n",
      "Epoch: 024 loss_train: 1.5994 acc_train: 0.3079 loss_val: 1.6232 acc_val: 0.2768 time: 0.4854s\n",
      "Epoch: 025 loss_train: 1.5706 acc_train: 0.3196 loss_val: 1.5918 acc_val: 0.3026 time: 0.5405s\n",
      "Epoch: 026 loss_train: 1.5316 acc_train: 0.3516 loss_val: 1.5587 acc_val: 0.3284 time: 0.4953s\n",
      "Epoch: 027 loss_train: 1.5038 acc_train: 0.3713 loss_val: 1.5229 acc_val: 0.3506 time: 0.4891s\n",
      "Epoch: 028 loss_train: 1.4652 acc_train: 0.3959 loss_val: 1.4860 acc_val: 0.3561 time: 0.5051s\n",
      "Epoch: 029 loss_train: 1.4247 acc_train: 0.4113 loss_val: 1.4486 acc_val: 0.3616 time: 0.4979s\n",
      "Epoch: 030 loss_train: 1.3819 acc_train: 0.4304 loss_val: 1.4109 acc_val: 0.3838 time: 0.5107s\n",
      "Epoch: 031 loss_train: 1.3349 acc_train: 0.4409 loss_val: 1.3715 acc_val: 0.4244 time: 0.4884s\n",
      "Epoch: 032 loss_train: 1.2880 acc_train: 0.4809 loss_val: 1.3297 acc_val: 0.4760 time: 0.6157s\n",
      "Epoch: 033 loss_train: 1.2584 acc_train: 0.4975 loss_val: 1.2866 acc_val: 0.5111 time: 0.4896s\n",
      "Epoch: 034 loss_train: 1.2113 acc_train: 0.5413 loss_val: 1.2439 acc_val: 0.5554 time: 0.4775s\n",
      "Epoch: 035 loss_train: 1.1770 acc_train: 0.5597 loss_val: 1.2026 acc_val: 0.5738 time: 0.5288s\n",
      "Epoch: 036 loss_train: 1.1228 acc_train: 0.5942 loss_val: 1.1622 acc_val: 0.5830 time: 0.4774s\n",
      "Epoch: 037 loss_train: 1.0869 acc_train: 0.6176 loss_val: 1.1243 acc_val: 0.6052 time: 0.5374s\n",
      "Epoch: 038 loss_train: 1.0372 acc_train: 0.6349 loss_val: 1.0855 acc_val: 0.6402 time: 0.5564s\n",
      "Epoch: 039 loss_train: 0.9998 acc_train: 0.6638 loss_val: 1.0475 acc_val: 0.6605 time: 0.4545s\n",
      "Epoch: 040 loss_train: 0.9688 acc_train: 0.6952 loss_val: 1.0100 acc_val: 0.6808 time: 0.5360s\n",
      "Epoch: 041 loss_train: 0.9228 acc_train: 0.7118 loss_val: 0.9736 acc_val: 0.6993 time: 0.5149s\n",
      "Epoch: 042 loss_train: 0.8874 acc_train: 0.7186 loss_val: 0.9395 acc_val: 0.7140 time: 0.4874s\n",
      "Epoch: 043 loss_train: 0.8476 acc_train: 0.7512 loss_val: 0.9083 acc_val: 0.7232 time: 0.5265s\n",
      "Epoch: 044 loss_train: 0.8208 acc_train: 0.7568 loss_val: 0.8799 acc_val: 0.7269 time: 0.4797s\n",
      "Epoch: 045 loss_train: 0.7858 acc_train: 0.7660 loss_val: 0.8508 acc_val: 0.7269 time: 0.4787s\n",
      "Epoch: 046 loss_train: 0.7514 acc_train: 0.7672 loss_val: 0.8211 acc_val: 0.7269 time: 0.4909s\n",
      "Epoch: 047 loss_train: 0.7245 acc_train: 0.7734 loss_val: 0.7962 acc_val: 0.7288 time: 0.5038s\n",
      "Epoch: 048 loss_train: 0.6950 acc_train: 0.7808 loss_val: 0.7738 acc_val: 0.7306 time: 0.5245s\n",
      "Epoch: 049 loss_train: 0.6702 acc_train: 0.7759 loss_val: 0.7566 acc_val: 0.7306 time: 0.4892s\n",
      "Epoch: 050 loss_train: 0.6422 acc_train: 0.7876 loss_val: 0.7424 acc_val: 0.7343 time: 0.4756s\n",
      "Epoch: 051 loss_train: 0.6194 acc_train: 0.7826 loss_val: 0.7217 acc_val: 0.7288 time: 0.5346s\n",
      "Epoch: 052 loss_train: 0.6018 acc_train: 0.7857 loss_val: 0.7016 acc_val: 0.7362 time: 0.5141s\n",
      "Epoch: 053 loss_train: 0.5750 acc_train: 0.7882 loss_val: 0.6831 acc_val: 0.7362 time: 0.4986s\n",
      "Epoch: 054 loss_train: 0.5596 acc_train: 0.7857 loss_val: 0.6686 acc_val: 0.7380 time: 0.4990s\n",
      "Epoch: 055 loss_train: 0.5295 acc_train: 0.7993 loss_val: 0.6565 acc_val: 0.7380 time: 0.4797s\n",
      "Epoch: 056 loss_train: 0.5133 acc_train: 0.8097 loss_val: 0.6454 acc_val: 0.7528 time: 0.5068s\n",
      "Epoch: 057 loss_train: 0.5014 acc_train: 0.8091 loss_val: 0.6304 acc_val: 0.7712 time: 0.5558s\n",
      "Epoch: 058 loss_train: 0.4797 acc_train: 0.8288 loss_val: 0.6152 acc_val: 0.7860 time: 0.4920s\n",
      "Epoch: 059 loss_train: 0.4745 acc_train: 0.8381 loss_val: 0.5999 acc_val: 0.7989 time: 0.4952s\n",
      "Epoch: 060 loss_train: 0.4545 acc_train: 0.8362 loss_val: 0.5877 acc_val: 0.8081 time: 0.5004s\n",
      "Epoch: 061 loss_train: 0.4503 acc_train: 0.8467 loss_val: 0.5793 acc_val: 0.8284 time: 0.5377s\n",
      "Epoch: 062 loss_train: 0.4191 acc_train: 0.8658 loss_val: 0.5699 acc_val: 0.8524 time: 0.5463s\n",
      "Epoch: 063 loss_train: 0.4192 acc_train: 0.8750 loss_val: 0.5604 acc_val: 0.8542 time: 0.5165s\n",
      "Epoch: 064 loss_train: 0.4100 acc_train: 0.8768 loss_val: 0.5504 acc_val: 0.8598 time: 0.5674s\n",
      "Epoch: 065 loss_train: 0.3874 acc_train: 0.8855 loss_val: 0.5396 acc_val: 0.8616 time: 0.5016s\n",
      "Epoch: 066 loss_train: 0.3836 acc_train: 0.8879 loss_val: 0.5312 acc_val: 0.8672 time: 0.5704s\n",
      "Epoch: 067 loss_train: 0.3703 acc_train: 0.8904 loss_val: 0.5251 acc_val: 0.8653 time: 0.5406s\n",
      "Epoch: 068 loss_train: 0.3581 acc_train: 0.8978 loss_val: 0.5200 acc_val: 0.8653 time: 0.5395s\n",
      "Epoch: 069 loss_train: 0.3387 acc_train: 0.9009 loss_val: 0.5145 acc_val: 0.8690 time: 0.5783s\n",
      "Epoch: 070 loss_train: 0.3449 acc_train: 0.8910 loss_val: 0.5042 acc_val: 0.8672 time: 0.5950s\n",
      "Epoch: 071 loss_train: 0.3311 acc_train: 0.9089 loss_val: 0.4962 acc_val: 0.8690 time: 0.5135s\n",
      "Epoch: 072 loss_train: 0.3236 acc_train: 0.9015 loss_val: 0.4904 acc_val: 0.8708 time: 0.5003s\n",
      "Epoch: 073 loss_train: 0.3027 acc_train: 0.9187 loss_val: 0.4865 acc_val: 0.8708 time: 0.5012s\n",
      "Epoch: 074 loss_train: 0.2944 acc_train: 0.9187 loss_val: 0.4808 acc_val: 0.8727 time: 0.5604s\n",
      "Epoch: 075 loss_train: 0.2926 acc_train: 0.9132 loss_val: 0.4718 acc_val: 0.8708 time: 0.6063s\n",
      "Epoch: 076 loss_train: 0.2814 acc_train: 0.9212 loss_val: 0.4656 acc_val: 0.8727 time: 0.5052s\n",
      "Epoch: 077 loss_train: 0.2792 acc_train: 0.9218 loss_val: 0.4614 acc_val: 0.8745 time: 0.4979s\n",
      "Epoch: 078 loss_train: 0.2647 acc_train: 0.9236 loss_val: 0.4627 acc_val: 0.8745 time: 0.6012s\n",
      "Epoch: 079 loss_train: 0.2622 acc_train: 0.9187 loss_val: 0.4642 acc_val: 0.8727 time: 0.5716s\n",
      "Epoch: 080 loss_train: 0.2545 acc_train: 0.9249 loss_val: 0.4650 acc_val: 0.8727 time: 0.5719s\n",
      "Epoch: 081 loss_train: 0.2395 acc_train: 0.9292 loss_val: 0.4641 acc_val: 0.8708 time: 0.4751s\n",
      "Epoch: 082 loss_train: 0.2338 acc_train: 0.9347 loss_val: 0.4659 acc_val: 0.8708 time: 0.5625s\n",
      "Epoch: 083 loss_train: 0.2359 acc_train: 0.9298 loss_val: 0.4628 acc_val: 0.8708 time: 0.5425s\n",
      "Epoch: 084 loss_train: 0.2326 acc_train: 0.9317 loss_val: 0.4560 acc_val: 0.8727 time: 0.5031s\n",
      "Epoch: 085 loss_train: 0.2214 acc_train: 0.9298 loss_val: 0.4533 acc_val: 0.8708 time: 0.4552s\n",
      "Epoch: 086 loss_train: 0.2169 acc_train: 0.9335 loss_val: 0.4578 acc_val: 0.8690 time: 0.4663s\n",
      "Epoch: 087 loss_train: 0.2134 acc_train: 0.9304 loss_val: 0.4631 acc_val: 0.8690 time: 0.4561s\n",
      "Epoch: 088 loss_train: 0.2179 acc_train: 0.9323 loss_val: 0.4640 acc_val: 0.8690 time: 0.4339s\n",
      "Epoch: 089 loss_train: 0.2039 acc_train: 0.9353 loss_val: 0.4618 acc_val: 0.8690 time: 0.4319s\n",
      "Epoch: 090 loss_train: 0.2072 acc_train: 0.9353 loss_val: 0.4594 acc_val: 0.8690 time: 0.4546s\n",
      "Epoch: 091 loss_train: 0.1962 acc_train: 0.9366 loss_val: 0.4598 acc_val: 0.8708 time: 0.4398s\n",
      "Epoch: 092 loss_train: 0.1969 acc_train: 0.9353 loss_val: 0.4623 acc_val: 0.8727 time: 0.4616s\n",
      "Epoch: 093 loss_train: 0.1913 acc_train: 0.9433 loss_val: 0.4661 acc_val: 0.8708 time: 0.5007s\n",
      "Epoch: 094 loss_train: 0.1876 acc_train: 0.9390 loss_val: 0.4721 acc_val: 0.8708 time: 0.4564s\n",
      "Epoch: 095 loss_train: 0.1800 acc_train: 0.9446 loss_val: 0.4730 acc_val: 0.8727 time: 0.5048s\n",
      "Epoch: 096 loss_train: 0.1834 acc_train: 0.9360 loss_val: 0.4719 acc_val: 0.8708 time: 0.4563s\n",
      "Epoch: 097 loss_train: 0.1763 acc_train: 0.9409 loss_val: 0.4721 acc_val: 0.8672 time: 0.4505s\n",
      "Epoch: 098 loss_train: 0.1706 acc_train: 0.9366 loss_val: 0.4741 acc_val: 0.8672 time: 0.4222s\n",
      "Epoch: 099 loss_train: 0.1718 acc_train: 0.9421 loss_val: 0.4785 acc_val: 0.8708 time: 0.4743s\n",
      "Epoch: 100 loss_train: 0.1697 acc_train: 0.9421 loss_val: 0.4838 acc_val: 0.8708 time: 0.4636s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 50.8804s\n",
      "\n",
      "Test set results: loss= 0.4292 accuracy= 0.8819\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print()\n",
    "\n",
    "# Testing\n",
    "embeddings_test = test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
